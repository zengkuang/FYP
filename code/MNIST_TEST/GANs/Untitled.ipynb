{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder for Minimization in Image Data Compression Loss\n",
    "\n",
    "This model will be the high-level feature extractor for images which will be further sent into CDCGAN as the condition feature for image inpainting and restoration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from random import randint\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST Dataset\n",
    "\n",
    "The dataset composes three elements:\n",
    "\n",
    "mnist.train.images of shape 5.5k * 28 * 28\n",
    "\n",
    "mnist.train.labels of shape 5.5k \n",
    "\n",
    "mnist.train.validationn shape 5k * 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28)\n",
      "(55000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAEMCAYAAAAiW8hnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAE4NJREFUeJzt3X2QVfV9x/H3J4jRCgJWg/hIUaNo0xAljk5MSo2m4jRRp1ZDaiXazCaOzoDRPxxGG5KapMmI2owdHRKpmkQyWg2ik6QCIYOUGAsZAvhQTTMSBVxEZVnUYMVv/zhnf96s7Ln37n04d5fPa2Znd+/3PHzv2d3PPQ+/PVcRgZkZwPvKbsDMOocDwcwSB4KZJQ4EM0scCGaWOBDMLHEg9CNprqQflLTuSZJ2NnvavYmkH0ia2+x5Jc2U9NNGehsKhn0gSLpK0mpJuyTd1a82TdKLg1zuUZJ2VnyEpNcrvv94vcuMiN9FxKhmT1uv/A/jLUm9+cd6SV+XdGAdy3hR0rQGelgp6fODnb/ZIuLuiJhe73yS9pN0l6QdkrZImtWK/ppl2AcCsBm4EVjQzIVGxO8jYlTfR/7whysee6z/PJJGNLOHFvtGRIwGDgH+Efg48Jik/ctta8j5Z2AicBRwNjBH0lmldlRg2AdCRDwYEYuAVyofl3QA8FPgsIpX9cPy8r6S7slfHZ+UNHUw685faf9N0s8kvQ58XNJnJK3NXzF+L+mGiumPlRQV36+U9FVJq/JefibpoHqnzeuX5evbJmlOra/gEfGHiHgC+DRwKDAzX95xkpZLejVf5vcljclrC4HDgJ/m2/XLkt4n6T8kvSRpu6RfSJo8iG1ay3IOkbQs3w7LJR1ZMf+JkpbmfT8j6W9rXO8XJP2ioofvSNoqqUfSOkknDjDrpcDXImJ7RGwge2H6fL3Pu12GfSAMJCJeB6YDmyte1Tfn5c8APwLGAouB2xpY1eeArwKjgV8CO4G/z5f9aWCWpL+pMv9MYDxwAPDleqeV9CHgO8BngcPJXvUPredJREQPsIxsTwFAZHtehwInApOAG/JpZ5DtmU3Pt+vN+TyPAMfl82wAvl9PDxWqLecS4J+Ag4Gn+uqSRgFLgHuAD5D9HOZLOr7O9U8HTst7GEe2XV/tP5GkQ/L1/Kbi4d8AJ9W5vrbZawOhipUR8ZOI2E32y/ThBpb144j4ZUS8ExG7IuLnEfFk/v1vyILnLwvmvzMinouIN4D7gSmDmPbvgEURsSoidgHXD/K5bAYOAoiIZyNiWUS8FRFbgVuKnkf+fO+KiN6I+AMwFzgl31OrWY3LeTgi/it/rnOAT0iaAJwHPBsR90TE2xGxBlgEXFhPD8D/AQcCJ+Q9PRURL+1hur5DyZ6Kx3rIXhw6kgNhzyp/uG8A+0naZ5DLeqHyG0mn57u5L0vqAb5A9kpWay9FJxIHmvawyj7yvaPXaui9v8PJXwklHSrpPkmbJO0A7qLgeUgaIenbkn6XT//bvFT03Ae7nMrn2kP2R3gYcDTwsfxQY7uk7cDFwIR6eoiIR4E7gNuBbkl3SNrTH3nfVaDKk7EHAr31rK+d9vZAaMe/evZfx4+AB4AjI2IM8D2y3e9W2gIc0fdN/mo6rp4F5FcYzgT6TpZ+C9gFfCgiDiQ7Lq58Hv2f96XAufkyxgDH9i26nj5qXE7lOYMx+XSbyYJiWUSMrfgYFRFX1dkDEXFrRJwM/DnZIdN7DuUi4mXgZf54D/PDwJP1rq9dhn0gSNpH0n7ACGBEfhmo79W+G/jTvpNhbTIaeDUi/iDpNLLjz1a7Hzhf0mmS9gW+VuuMkt6fn1R9iOyX+568NBp4HejJT9pd22/WbrLzClRMv4vs5O6fAF+vYfUj859X38fIGpfz6XxP7P1k5zkei4gtZOeDTpL0OUkj849T6z2HkM9zav579DrwFvDOAJPfA9wgaWx+4vFysr2pjjTsA4HsePlN4Dqyk01v5o8REc8AC4Hf5buQhw24lOa5AvimpF6y49v7Wr3CiFgHXE0WDJvJ/pheIfvDGsicvMdXgLuBx4GP5ecnAL4CnEq2O76YbK+n0jeAr+bbdTbw7/m6N5O9Qq6qofX5ZD+vvo/v1ricH5AFwTbgL8j2KvoOH/6a7PdgC9kh1jeB99fQS6WxwJ3AduD5fFk3DzDtDWR7Ji8APwe+GRFL61xf+0RE2z+Ac4D/ITv+u66MHqr09zywHlgLrO6AfhYAW4ENFY8dRHbG/Ln887g6lncg2SvakS3sby6wKd+Ga4FzS9x+RwLLya44PAnManQbtqm/tm/DMp78COB/yXYn9yW7DHNiWb8sA/T4PHBw2X1U9PMJ4OR+f3Df7gtTsr2fb1VZxmfIdrFHkb3SNi3oBuhvLnBt2dsu72UCcHL+9WjgWbLj/rq2YQn9tX0blnHIcCrw28iG3r5FdpLtvBL6GDIiYgXvvc59HtmuPPnn86ss5gKy3ewXyUbOzWhxfx0jIrZExK/zr3uBp8mumNS7DdvdX9uVEQiH88eX4l6kpCdfIIBHJa2R1FV2MwMYH9mJMsiOhccXTRwRl8W7Z9bPjojnWt8iV+Wj+BZIquuqRqtImgh8BPgVdW7DdujXH7R5G+4NJxUH44zILilNB66U9ImyGyoS2b5mp90t93bgGLLBUVuAeeW2k0YqPgDMjogdlbVO2IZ76K/t27CMQNhExXVisuvjm0roY0ARsSn/vBX4MdlhTqfpzkffkX/eWnI/fyQiuiNid0S8Q3bOotRtmF+yfAD4YUQ8mD/cMdtwT/2VsQ3LCIT/Bo6T9Gf5NfHPkl226giSDugbdZYP4PkU2Xj5TrOY/B+N8s8PldjLe/T9oeUuoMRtKElklwmfjnf/rwI6ZBsO1F8Z21D5mc22knQucCvZFYcFEVHLIJW2kDSJbK8AYB/g3rL7U/bfg9PIhud2k40BWEQ2huEoYCNwUUSUcmJvgP6mke3qBtlVmy9WHK+3u78zyEZYrufdAURzyI7TS9+GBf3NoM3bsJRAMLPO5JOKZpY4EMwscSCYWeJAMLPEgWBmSamB0MHDggH316hO7q+Te4Py+it7D6Gjfyi4v0Z1cn+d3BuU1F/ZgWBmHaShgUmSzgH+lWzE4fci4l+qTO9RUGYliYiq968cdCAoexeiZ8nejeZFsv9RmBERTxXM40AwK0ktgdDIIYNvdGI2zDQSCEPhRidmVofBvvlIzfLLJ51+RtfMaCwQarrRSUTMJ7udts8hmHW4Rg4ZOvpGJ2ZWv0HvIUTE25KuAv6Td2900rFvUWVm1bX1Bik+ZDArT6svO5rZMONAMLPEgWBmiQPBzBIHgpklDgQzSxwIZpY4EMwscSCYWeJAMLPEgWBmiQPBzBIHgpklDgQzSxwIZpY4EMwscSCYWeJAMLPEgWBmiQPBzBIHgpklDgQzSxwIZpY4EMwscSCYWeJAMLPEgWBmiQPBzBIHgpklDgQzSxwIZpbs08jMkp4HeoHdwNsRMbUZTZlZORoKhNxfRcS2JizHzErmQwYzSxoNhAAelbRGUlczGjKz8jR6yHBGRGyS9AFgiaRnImJF5QR5UDgszIYARURzFiTNBXZGxE0F0zRnZWZWt4hQtWkGfcgg6QBJo/u+Bj4FbBjs8sysfI0cMowHfiypbzn3RsTPmtKVmZWiaYcMNa3MhwxmpWnpIYOZDT8OBDNLHAhmljgQzCxxIJhZ4kAws6QZ/+1oHeKyyy4rrFe7xPzKK68U1idPnlxYX7VqVWF95cqVhXUrn/cQzCxxIJhZ4kAws8SBYGaJA8HMEgeCmSUOBDNLhtU4hBkzZhTWTz755MJ6tev4nW7s2LENzb979+7C+r777ltYf/PNNwvrb7zxRmF9/fr1hfWLLrqosP7yyy8X1q067yGYWeJAMLPEgWBmiQPBzBIHgpklDgQzSxwIZpYMqduwz5s3r7A+a9aswvqIESMaWb2VbPny5YX1auNQuru7m9nOkOPbsJtZXRwIZpY4EMwscSCYWeJAMLPEgWBmiQPBzJIhNQ7hhRdeKKwfccQRhfV169YV1qv9P3+rVXvfgkWLFrWpk8E5++yzC+uXXnppYX3ixIkNrb/aOIWLL764sD7c76fQlHEIkhZI2ippQ8VjB0laIum5/PO4Rps1s/LVcshwF3BOv8euA5ZFxHHAsvx7MxviqgZCRKwAXu338HnA3fnXdwPnN7kvMyvBYE8qjo+ILfnXLwHjm9SPmZWo4ZusRkQUnSyU1AV0NboeM2u9we4hdEuaAJB/3jrQhBExPyKmRsTUQa7LzNpksIGwGJiZfz0TeKg57ZhZmaqOQ5C0EJgGHAx0A18BFgH3AUcBG4GLIqL/icc9LauhcQgf/OAHC+snnXRSYX3p0qWF9d7e3rp7stpNmjSpsP7II48U1idPntzQ+q+99trCerX7bQx1tYxDqHoOISIGuuvEJ+vuyMw6mocum1niQDCzxIFgZokDwcwSB4KZJQ4EM0uG1P0QbHi78MILC+v3339/Q8vftm1bYf2QQw5paPmdzu/LYGZ1cSCYWeJAMLPEgWBmiQPBzBIHgpklDgQzSxwIZpY4EMwscSCYWeJAMLPEgWBmiQPBzBIHgpklDgQzSxp+KzezWl1xxRWF9Y9+9KMtXf9+++1XWD/llFMK62vWrGlmOx3JewhmljgQzCxxIJhZ4kAws8SBYGaJA8HMEgeCmSV+X4ZhZMKECYX1Sy65pLA+e/bsZrbzHtX6k6q+bUBL7dixo7A+ZsyYNnXSGk15XwZJCyRtlbSh4rG5kjZJWpt/nNtos2ZWvloOGe4CztnD47dExJT84yfNbcvMylA1ECJiBfBqG3oxs5I1clLxKknr8kOKcU3ryMxKM9hAuB04BpgCbAHmDTShpC5JqyWtHuS6zKxNBhUIEdEdEbsj4h3gu8CpBdPOj4ipETF1sE2aWXsMKhAkVV4/ugDYMNC0ZjZ0VL0fgqSFwDTgYEkvAl8BpkmaAgTwPPDFFva41zjrrLMK69X+X7+rq6uwPmnSpLp72pssWLCg7BZKVzUQImLGHh6+swW9mFnJPHTZzBIHgpklDgQzSxwIZpY4EMwscSCYWeL3ZWiiY489trB+xx13FNbPPPPMwnqr7xewcePGwvprr73W0PKvv/76wvquXbsK67fddlth/fjjj6+7p0qbN29uaP7hwHsIZpY4EMwscSCYWeJAMLPEgWBmiQPBzBIHgpklHodQh6uvvrqwfuWVVxbWjznmmML6zp07C+vbt28vrN96662F9WrX2VetWlVYrzZOodV6enoamr+3t7ew/vDDDze0/OHAewhmljgQzCxxIJhZ4kAws8SBYGaJA8HMEgeCmSUeh1CH008/vbBebZzB4sWLC+vz5g34jngArFixorA+1E2ZMqWwfvTRRze0/Gr3W3jmmWcaWv5w4D0EM0scCGaWOBDMLHEgmFniQDCzxIFgZokDwcwSj0Oow5e+9KXC+rp16wrrN954YzPbGXaqva/F+PHjG1r+0qVLG5p/b1B1D0HSkZKWS3pK0pOSZuWPHyRpiaTn8s/jWt+umbVSLYcMbwPXRMSJwGnAlZJOBK4DlkXEccCy/HszG8KqBkJEbImIX+df9wJPA4cD5wF355PdDZzfqibNrD3qOqkoaSLwEeBXwPiI2JKXXgIaO8Azs9LVfFJR0ijgAWB2ROyofOPRiAhJMcB8XUBXo42aWevVtIcgaSRZGPwwIh7MH+6WNCGvTwC27mneiJgfEVMjYmozGjaz1qnlKoOAO4GnI+LmitJiYGb+9Uzgoea3Z2btpIg97um/O4F0BvAYsB54J394Dtl5hPuAo4CNwEUR8WqVZRWvzPZqN910U2H9mmuuKaxXe9+K6dOnF9Yff/zxwvpQFxGqNk3VcwgRsRIYaEGfrLcpM+tcHrpsZokDwcwSB4KZJQ4EM0scCGaWOBDMLPH9EKxt1q9fX1g/4YQTGlr+o48+Wlgf7uMMmsF7CGaWOBDMLHEgmFniQDCzxIFgZokDwcwSB4KZJR6HYG0zceLEwvo++xT/Ovb09BTWb7nllnpbsn68h2BmiQPBzBIHgpklDgQzSxwIZpY4EMwscSCYWeJxCNY0M2bMKKzvv//+hfXe3t7CeldX8TsC+n4HjfMegpklDgQzSxwIZpY4EMwscSCYWeJAMLPEgWBmiSKifSuT2rcya7qRI0cW1p944onCerX3XVi4cGFh/fLLLy+sW7GIULVpqu4hSDpS0nJJT0l6UtKs/PG5kjZJWpt/nNuMps2sPLWMVHwbuCYifi1pNLBG0pK8dktE3NS69sysnaoGQkRsAbbkX/dKeho4vNWNmVn71XVSUdJE4CPAr/KHrpK0TtICSeMGmKdL0mpJqxvq1MxaruZAkDQKeACYHRE7gNuBY4ApZHsQ8/Y0X0TMj4ipETG1Cf2aWQvVFAiSRpKFwQ8j4kGAiOiOiN0R8Q7wXeDU1rVpZu1Qy1UGAXcCT0fEzRWPT6iY7AJgQ/PbM7N2quUqw8eAfwDWS1qbPzYHmCFpChDA88AXW9KhdYxqY1buvffewvratWsL60uWLCmsW+vVcpVhJbCnAQ0/aX47ZlYmD102s8SBYGaJA8HMEgeCmSUOBDNLHAhmlvh+CGZ7iabcD8HM9h4OBDNLHAhmljgQzCxxIJhZ4kAws8SBYGZJLfdDaKZtwMaK7w/OH+tU7q8xndxfJ/cGze/v6FomauvApPesXFrdyfdadH+N6eT+Ork3KK8/HzKYWeJAMLOk7ECYX/L6q3F/jenk/jq5Nyipv1LPIZhZZyl7D8HMOogDwcwSB4KZJQ4EM0scCGaW/D+EbiCX6iGQ+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print out several data in mnist\n",
    "trainimg = mnist.train.images\n",
    "train_y = mnist.train.labels\n",
    "nsample = 1\n",
    "randidx = np.random.randint(trainimg.shape[0], size=nsample)\n",
    "\n",
    "train_x = np.reshape(trainimg,(trainimg.shape[0],28,28))\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    curr_img   = np.reshape(trainimg[i, :], (28, 28)) # 28 by 28 matrix \n",
    "    curr_label = np.argmax(train_y[i] ) # Label\n",
    "    plt.matshow(curr_img, cmap=plt.get_cmap('gray'))\n",
    "    plt.title(\"\" + str(i + 1) + \"th Training Data \" \n",
    "              + \"Label is \" + str(curr_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Globals\n",
    "NUM_LABELS = 47\n",
    "rnd = np.random.RandomState(123)\n",
    "tf.set_random_seed(123)\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define utils\n",
    "\n",
    "def convert_image_data_to_float(image_raw):\n",
    "    img_float = tf.expand_dims(tf.cast(image_raw, tf.float32) / 255, axis=-1)\n",
    "    return img_float\n",
    "\n",
    "\n",
    "def visualize_ae(i, x, features, reconstructed_image):\n",
    "    '''\n",
    "    This might be helpful for visualizing your autoencoder outputs\n",
    "    :param i: index\n",
    "    :param x: original data\n",
    "    :param features: feature maps\n",
    "    :param reconstructed_image: autoencoder output\n",
    "    :return:\n",
    "    '''\n",
    "    plt.figure(0)\n",
    "    plt.imshow(x[i, :, :], cmap=\"gray\")\n",
    "    plt.savefig(\"./plts/origin/{}.jpg\".format(i))\n",
    "    plt.figure(1)\n",
    "    plt.imshow(reconstructed_image[i, :, :, 0], cmap=\"gray\")\n",
    "    plt.savefig(\"./plts/reconstructed/{}.jpg\".format(i))\n",
    "    plt.figure(2)\n",
    "    plt.imshow(np.reshape(features[i, :, :, :], (4, -1), order=\"F\"), cmap=\"gray\")\n",
    "    plt.savefig(\"./plts/fmap/{}.jpg\".format(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Necessary Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful layers \n",
    "\n",
    "def conv2d(input, name, kshape, strides=[1, 1, 1, 1], actv_fn = \"RELU\"):\n",
    "    with tf.name_scope(name):\n",
    "        W = tf.get_variable(name='w_'+name,\n",
    "                            shape=kshape,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "        b = tf.get_variable(name='b_' + name,\n",
    "                            shape=[kshape[3]],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "        out = tf.nn.conv2d(input,W,strides=strides, padding='SAME')\n",
    "        out = tf.nn.bias_add(out, b)\n",
    "        if actv_fn == \"RELU\":\n",
    "            out = tf.nn.relu(out)\n",
    "        elif actv_fn == 'SIGMOID':\n",
    "            out = tf.nn.sigmoid(out)\n",
    "        else:\n",
    "            print(\"No such activation function implemented!\")\n",
    "            exit(0)\n",
    "\n",
    "        return out\n",
    "# ---------------------------------\n",
    "def deconv2d(input, name, kshape, n_outputs, strides=[1, 1] ,actv_fn = \"RELU\"):\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        if actv_fn == \"RELU\":\n",
    "            out = tf.contrib.layers.conv2d_transpose(input,\n",
    "                                                 num_outputs= n_outputs,\n",
    "                                                 kernel_size=kshape,\n",
    "                                                 stride=strides,\n",
    "                                                 padding='SAME',\n",
    "                                                 weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(uniform=False),\n",
    "                                                 biases_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                                                 activation_fn=tf.nn.relu)\n",
    "        elif actv_fn == 'SIGMOID':\n",
    "            out = tf.contrib.layers.conv2d_transpose(input,\n",
    "                                                 num_outputs= n_outputs,\n",
    "                                                 kernel_size=kshape,\n",
    "                                                 stride=strides,\n",
    "                                                 padding='SAME',\n",
    "                                                 weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(uniform=False),\n",
    "                                                 biases_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                                                 activation_fn=tf.nn.sigmoid)\n",
    "        else:\n",
    "            print(\"No such activation funciton implemented!\")\n",
    "            exit(0)\n",
    "\n",
    "        return out\n",
    "#   ---------------------------------\n",
    "def maxpool2d(x,name,kshape=[1, 2, 2, 1], strides=[1, 2, 2, 1]):\n",
    "    with tf.name_scope(name):\n",
    "        out = tf.nn.max_pool(x,\n",
    "                             ksize=kshape, #size of window\n",
    "                             strides=strides,\n",
    "                             padding='SAME')\n",
    "        return out\n",
    "#   ---------------------------------\n",
    "def upsample(input, name, factor=[2,2]):\n",
    "    size = [int(input.shape[1] * factor[0]), int(input.shape[2] * factor[1])]\n",
    "    with tf.name_scope(name):\n",
    "        out = tf.image.resize_nearest_neighbor(input, size=size, align_corners=False, name=None)\n",
    "        return out\n",
    "#   ----------------------------------\n",
    "def fullyConnected(input, name, output_size, actv_fn = \"RELU\"):\n",
    "    with tf.name_scope(name):\n",
    "        input_size = input.shape[1:]\n",
    "        input_size = int(np.prod(input_size))\n",
    "        W = tf.get_variable(name='w_'+name,\n",
    "                            shape=[input_size, output_size],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "        b = tf.get_variable(name='b_'+name,\n",
    "                            shape=[output_size],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "        input = tf.reshape(input, [-1, input_size])\n",
    "        if actv_fn == \"RELU\":\n",
    "            out = tf.nn.relu(tf.add(tf.matmul(input, W), b))\n",
    "        elif actv_fn == \"SOFTMAX\":\n",
    "            out = tf.nn.softmax(tf.add(tf.matmul(input, W), b))\n",
    "        else:\n",
    "            print(\"Activation function not implemented!!\")\n",
    "            exit(0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description\n",
    "\n",
    "see [] for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model structures\n",
    "def CAE_encoder(x, name):\n",
    "\n",
    "    #print(x.shape)\n",
    "    c1 = conv2d(x, name=\"c1\", kshape =[3,3,1,32] )\n",
    "    #print(c1.shape)\n",
    "    p1 = maxpool2d(c1, name = \"p1\")\n",
    "    #print(p1.shape)\n",
    "    c2 = conv2d(p1, name = \"c2\", kshape =[3,3,32,64],strides = [1,2,2,1])\n",
    "    #print(c2.shape)\n",
    "    c3 = conv2d(c2, name = \"c3\", kshape =[3,3,64,64])\n",
    "    #print(c3.shape)\n",
    "    out = maxpool2d(c3, name = \"out\")\n",
    "    #print(out.shape)\n",
    "    return out\n",
    "\n",
    "def CAE_decoder(x, name):  \n",
    "\n",
    "    #print(x.shape)\n",
    "    #up3 = upsample(x, name = \"up3\")\n",
    "    up3 = tf.image.resize_nearest_neighbor(x, size=[7,7], align_corners=False, name=None)\n",
    "    #print(up3.shape)\n",
    "    dc3 = deconv2d(up3,name = \"dc3\", kshape = [3,3], n_outputs = 64)\n",
    "    #print(dc3.shape)\n",
    "    dc2 = deconv2d(dc3,name = \"dc2\", kshape = [3,3], n_outputs = 32,  strides = [2,2])\n",
    "    #print(dc2.shape)\n",
    "    up2 = upsample(dc2,name = \"up2\")\n",
    "    #print(up2.shape)\n",
    "    dc1 = deconv2d(up2,name = \"dc1\", kshape = [3,3], n_outputs = 1, actv_fn = \"SIGMOID\")\n",
    "    #print(dc1.shape)\n",
    "    return dc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training functions\n",
    "def train_cnn(x, y, placeholder_x, placeholder_y):\n",
    "    hs = [512,1024]\n",
    "    optimizers = [\"SGD\",\"ADAM\"]\n",
    "    lrs = [0.1,0.01,0.001]\n",
    "    ps = [0.5,1]\n",
    "    placeholder_xf = convert_image_data_to_float(placeholder_x)\n",
    "    encoder_output = CAE_encoder(placeholder_xf, \"Encoder\")\n",
    "\n",
    "    for p in ps: \n",
    "        \n",
    "        train_X = x[:int(x.shape[0]*p)]\n",
    "        train_Y = y[:int(y.shape[0]*p)]\n",
    "        val_X = x[int(x.shape[0]*p):]\n",
    "        val_Y = y[int(y.shape[0]*p):]\n",
    "        if p == 1: ##calculate accuracy on the whole training dataset when p = 1\n",
    "            val_X = train_X\n",
    "            val_Y = train_Y\n",
    "\n",
    "        n_epochs = 200\n",
    "        best_loss = 999 \n",
    "        best_lr = 0\n",
    "        best_opt = None\n",
    "        best_h = 0\n",
    "\n",
    "\n",
    "        for opt in optimizers:\n",
    "            for lr in lrs:\n",
    "                for h in hs:\n",
    "                    fc1 = fullyConnected(encoder_output, \"fc1_{}_{}_{}_{}\".format(p,opt,lr,h), h)\n",
    "                    out = fullyConnected(fc1, \"fc2_{}_{}_{}_{}\".format(p,opt,lr,h), 47, \"SOFTMAX\")\n",
    "                    \n",
    "                    #loss = tf.nn.softmax_cross_entropy_with_logits( logits=out , labels = placeholder_y)\n",
    "                    \n",
    "                    loss = tf.losses.sparse_softmax_cross_entropy(labels=placeholder_y, logits=out)\n",
    "                    #preds = tf.argmax(out, axis = 1)\n",
    "                    #accu = tf.reduce_mean(tf.equal(placeholder_y,preds))\n",
    "                    #accu, accu_op = tf.metrics.accuracy(labels = placeholder_y,predictions = preds)\n",
    "                    accu = tf.reduce_mean( tf.cast(tf.equal( placeholder_y, tf.cast( tf.argmax(out,1), tf.int32 ) ), tf.float32))      \n",
    "                    optimizer = None\n",
    "                    if opt == \"SGD\":\n",
    "                        optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr).minimize(loss)\n",
    "                    else:\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "\n",
    "\n",
    "                    saver = tf.train.Saver()\n",
    "                    \n",
    "                    \n",
    "                    with tf.Session() as sess:\n",
    "                        sess.run(tf.global_variables_initializer())\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        for epoch in range(n_epochs):\n",
    "                            avg_loss = 0\n",
    "                            n_batches = int( int(train_X.shape[0]) / batch_size )\n",
    "                            # Loop over all batches\n",
    "                            a = None\n",
    "                            for i in range(n_batches):\n",
    "                                batch_x = train_X[i*batch_size:(i+1)*batch_size]\n",
    "                                batch_y = train_Y[i*batch_size:(i+1)*batch_size]\n",
    "                                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                                _, l, a = sess.run([optimizer, loss, accu], feed_dict={placeholder_x: batch_x, placeholder_y :batch_y})\n",
    "                                # Compute average loss\n",
    "                                avg_loss += l / n_batches\n",
    "                            # Display logs per epoch step\n",
    "                            \n",
    "                            print('Epoch', epoch+1, ' / ', n_epochs, 'loss:', avg_loss, 'accu:', a)\n",
    "                            save_path = saver.save(sess, \"./cnn_model/{}/x-val_{}_{}_{}/model.ckpt\".format(p,opt,lr,h))       \n",
    "                        #evaluate on valset\n",
    "                        l = loss.eval(feed_dict = {placeholder_x: val_X, placeholder_y : val_Y})\n",
    "                        a = sess.run(accu,feed_dict = {placeholder_x: val_X, placeholder_y : val_Y})\n",
    "                        print(\"Validation loss: {}, Validation accu: {}\".format(l,a))\n",
    "                        if l < best_loss:\n",
    "                            best_loss = l\n",
    "                            best_lr = lr\n",
    "                            best_opt = opt\n",
    "                            best_h = h\n",
    "                            \n",
    "        #move the best model under ./ae_model\n",
    "        print(\"The best hyperparameters through holdout validation: p = {} lr = {} H = {} OPT = {}\".format(p, best_lr,best_h,best_opt))\n",
    "        shutil.copytree(\"./cnn_model/{}/x-val_{}_{}_{}\".format(p,best_opt,best_lr,best_h), \"./cnn_model/{}/best\".format(p))\n",
    "        f = open(\"./cnn_model/{}/hypers.txt\".format(p),\"w+\")\n",
    "        f.write(\"{} {} {}\".format(best_opt,best_lr,best_h))\n",
    "        f.close()\n",
    "                  \n",
    "def test_cnn(p, x, y, placeholder_x, placeholder_y):\n",
    "    opt = None\n",
    "    lr = None\n",
    "    h = None\n",
    "    if p == 1:\n",
    "        p = int(p)\n",
    "    f = open(\"./cnn_model/{}/hypers.txt\".format(p),\"r\")\n",
    "    if f.mode == 'r':\n",
    "        opt, lr, h = f.read().split(\" \")\n",
    "    f.close()\n",
    "\n",
    "    lr = float(lr)\n",
    "    h = int(h)\n",
    "        \n",
    "    placeholder_xf = convert_image_data_to_float(placeholder_x)\n",
    "    encoder_output = CAE_encoder(placeholder_xf, 'Encoder')\n",
    "    fc1 = fullyConnected(encoder_output, \"fc1_{}_{}_{}_{}\".format(p,opt,lr,h), h)\n",
    "    out = fullyConnected(fc1, \"fc2_{}_{}_{}_{}\".format(p,opt,lr,h), 47, \"SOFTMAX\")\n",
    "                    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=placeholder_y, logits=out)\n",
    "    accu = tf.reduce_mean( tf.cast(tf.equal( placeholder_y, tf.cast( tf.argmax(out,1), tf.int32 ) ), tf.float32))      \n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    result_accracy = None\n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, \"./cnn_model/{}/best/model.ckpt\".format(p))\n",
    "        print(\"Model restored.\")\n",
    "        loss = loss.eval(feed_dict = {placeholder_x: x, placeholder_y:y})\n",
    "        result_accuracy = sess.run(accu, feed_dict ={placeholder_x: x, placeholder_y:y})\n",
    "        print(\"P = {}\".format(p))\n",
    "        print(\"Loss: {}\".format(loss))\n",
    "        print(\"Accuracy: {}\".format(result_accuracy))\n",
    "            \n",
    "    return result_accuracy\n",
    "\n",
    "def test_cnn_pretrained(p, x, y, placeholder_x, placeholder_y):\n",
    "    opt = None\n",
    "    lr = None\n",
    "    h = None\n",
    "    if p == 1:\n",
    "        p = int(p)\n",
    "    f = open(\"./cnn_model_pretrained/{}/hypers.txt\".format(p),\"r\")\n",
    "    if f.mode == 'r':\n",
    "        opt, lr, h = f.read().split(\" \")\n",
    "    f.close()\n",
    "\n",
    "    lr = float(lr)\n",
    "    h = int(h)\n",
    "        \n",
    "    placeholder_xf = convert_image_data_to_float(placeholder_x)\n",
    "    # TODO: implement autoencoder training\n",
    "    encoder_output = CAE_encoder(placeholder_xf,'Encoder')\n",
    "    fc1 = fullyConnected(encoder_output, \"fc1_{}_{}_{}_{}\".format(p,opt,lr,h), h)\n",
    "    out = fullyConnected(fc1, \"fc2_{}_{}_{}_{}\".format(p,opt,lr,h), 47, \"SOFTMAX\")\n",
    "                    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=placeholder_y, logits=out)\n",
    "    accu = tf.reduce_mean( tf.cast(tf.equal( placeholder_y, tf.cast( tf.argmax(out,1), tf.int32 ) ), tf.float32))      \n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    result_accracy = None\n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, \"./cnn_model_pretrained/{}/best/model.ckpt\".format(p))\n",
    "        print(\"Model restored.\")\n",
    "        loss = loss.eval(feed_dict = {placeholder_x: x, placeholder_y:y})\n",
    "        result_accuracy = sess.run(accu, feed_dict ={placeholder_x: x, placeholder_y:y})\n",
    "        print(\"P = {}\".format(p))\n",
    "        print(\"Loss: {}\".format(loss))\n",
    "        print(\"Accuracy: {}\".format(result_accuracy))\n",
    "            \n",
    "    return result_accuracy\n",
    "\n",
    "def train_cnn_pretrained(x, y, placeholder_x, placeholder_y):\n",
    "    hs = [512,1024]\n",
    "    optimizers = [\"SGD\",\"ADAM\"]\n",
    "    lrs = [0.1,0.01,0.001]\n",
    "    ps = [0.5,1]\n",
    "    placeholder_xf = convert_image_data_to_float(placeholder_x)\n",
    "    encoder_output = CAE_encoder(placeholder_xf, \"Encoder\")\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, \"./ae_model/best/model.ckpt\")\n",
    "        print(\"Model restored.\")\n",
    "    \n",
    "    for p in ps: \n",
    "        \n",
    "        train_X = x[:int(x.shape[0]*p)]\n",
    "        train_Y = y[:int(y.shape[0]*p)]\n",
    "        val_X = x[int(x.shape[0]*p):]\n",
    "        val_Y = y[int(y.shape[0]*p):]\n",
    "        if p == 1: ##calculate accuracy on the whole training dataset when p = 1\n",
    "            val_X = train_X\n",
    "            val_Y = train_Y\n",
    "\n",
    "        n_epochs = 200\n",
    "        best_loss = 999 \n",
    "        best_lr = 0\n",
    "        best_opt = None\n",
    "        best_h = 0\n",
    "\n",
    "\n",
    "        for opt in optimizers:\n",
    "            for lr in lrs:\n",
    "                for h in hs:\n",
    "                    fc1 = fullyConnected(encoder_output, \"fc1_{}_{}_{}_{}\".format(p,opt,lr,h), h)\n",
    "                    out = fullyConnected(fc1, \"fc2_{}_{}_{}_{}\".format(p,opt,lr,h), 47, \"SOFTMAX\")\n",
    "                    \n",
    "                    #loss = tf.nn.softmax_cross_entropy_with_logits( logits=out , labels = placeholder_y)\n",
    "                    \n",
    "                    loss = tf.losses.sparse_softmax_cross_entropy(labels=placeholder_y, logits=out)\n",
    "                    #preds = tf.argmax(out, axis = 1)\n",
    "                    #accu = tf.reduce_mean(tf.equal(placeholder_y,preds))\n",
    "                    #accu, accu_op = tf.metrics.accuracy(labels = placeholder_y,predictions = preds)\n",
    "                    accu = tf.reduce_mean( tf.cast(tf.equal( placeholder_y, tf.cast( tf.argmax(out,1), tf.int32 ) ), tf.float32))      \n",
    "                    optimizer = None\n",
    "                    if opt == \"SGD\":\n",
    "                        optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr).minimize(loss)\n",
    "                    else:\n",
    "                        optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "\n",
    "\n",
    "                    saver = tf.train.Saver()\n",
    "                    \n",
    "                    \n",
    "                    with tf.Session() as sess:\n",
    "                        sess.run(tf.global_variables_initializer())\n",
    "                        sess.run(tf.local_variables_initializer())\n",
    "                        for epoch in range(n_epochs):\n",
    "                            avg_loss = 0\n",
    "                            n_batches = int( int(train_X.shape[0]) / batch_size )\n",
    "                            # Loop over all batches\n",
    "                            a = None\n",
    "                            for i in range(n_batches):\n",
    "                                batch_x = train_X[i*batch_size:(i+1)*batch_size]\n",
    "                                batch_y = train_Y[i*batch_size:(i+1)*batch_size]\n",
    "                                # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                                _, l, a = sess.run([optimizer, loss, accu], feed_dict={placeholder_x: batch_x, placeholder_y :batch_y})\n",
    "                                # Compute average loss\n",
    "                                avg_loss += l / n_batches\n",
    "                            # Display logs per epoch step\n",
    "                            \n",
    "                            print('Epoch', epoch+1, ' / ', n_epochs, 'loss:', avg_loss, 'accu:', a)\n",
    "                            save_path = saver.save(sess, \"./cnn_model_pretrained/{}/x-val_{}_{}_{}/model.ckpt\".format(p,opt,lr,h))       \n",
    "                        #evaluate on valset\n",
    "                        l = loss.eval(feed_dict = {placeholder_x: val_X, placeholder_y : val_Y})\n",
    "                        a = sess.run(accu,feed_dict = {placeholder_x: val_X, placeholder_y : val_Y})\n",
    "                        print(\"Validation loss: {}, Validation accu: {}\".format(l,a))\n",
    "                        if l < best_loss:\n",
    "                            best_loss = l\n",
    "                            best_lr = lr\n",
    "                            best_opt = opt\n",
    "                            best_h = h\n",
    "        #move the best model under ./ae_model\n",
    "        print(\"The best hyperparameters through holdout validation: p= {} lr = {} H = {} OPT = {}\".format(p,best_lr,best_h,best_opt))\n",
    "        shutil.copytree(\"./cnn_model_pretrained/{}/x-val_{}_{}_{}\".format(p,best_opt,best_lr,best_h), \"./cnn_model_pretrained/{}/best\".format(p))\n",
    "        f = open(\"./cnn_model_pretrained/{}/hypers.txt\".format(p),\"w+\")\n",
    "        f.write(\"{} {} {}\".format(best_opt,best_lr,best_h))\n",
    "        f.close()\n",
    "\n",
    "def train_ae(x, placeholder_x):\n",
    "    lrs = [0.1,0.01,0.001,0.0001]\n",
    "    best_loss = 999\n",
    "    best_lr = 0\n",
    "    #x-val\n",
    "    \n",
    "    placeholder_xf = convert_image_data_to_float(placeholder_x)\n",
    "    # TODO: implement autoencoder training\n",
    "    encoder_output = CAE_encoder(placeholder_xf, 'Encoder')\n",
    "    decoder_output = CAE_decoder(encoder_output, 'Decoder')\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(tf.subtract(decoder_output, placeholder_xf)))\n",
    "        \n",
    "    \n",
    "\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"MSE_loss\", loss)\n",
    "\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    n_epochs = 200\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    logs_path = \"./logs\"\n",
    "        \n",
    "    for lr in lrs:\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate = lr).minimize(loss)\n",
    "        #random shuffle\n",
    "        np.random.shuffle(x)\n",
    "        val_len = int(x.shape[0]/5)\n",
    "        val_X = x[:val_len]\n",
    "        train_X = x[val_len:]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            # create log writer object\n",
    "            writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                avg_loss = 0\n",
    "                n_batches = int( int(train_X.shape[0]) / batch_size )\n",
    "                # Loop over all batches\n",
    "                for i in range(n_batches):\n",
    "\n",
    "                    batch_x = train_X[i*batch_size:(i+1)*batch_size]\n",
    "                    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                    _, l, summary = sess.run([optimizer, loss, merged_summary_op], feed_dict={placeholder_x: batch_x})\n",
    "                    # Compute average loss\n",
    "                    avg_loss += l / n_batches\n",
    "                    # write log\n",
    "                    writer.add_summary(summary, epoch * n_batches + i)\n",
    "                    #print(avg_loss)\n",
    "                # Display logs per epoch step\n",
    "\n",
    "                print('Epoch', epoch+1, ' / ', n_epochs, 'loss:', avg_loss)\n",
    "                save_path = saver.save(sess, \"./ae_model/x-val_{}/model.ckpt\".format(lr))       \n",
    "            #evaluate on valset\n",
    "            l = loss.eval(feed_dict = {placeholder_x: val_X})\n",
    "            print(\"Validation loss: {}\".format(l))\n",
    "            if l < best_loss:\n",
    "                best_loss = l\n",
    "                best_lr = lr\n",
    "    \n",
    "    #move the best model under ./ae_model\n",
    "    print(\"The best learning rate through holdout validation:{}\".format(best_lr))\n",
    "    shutil.copytree(\"./ae_model/x-val_{}\".format(best_lr), \"./ae_model/best\")\n",
    "        \n",
    "def evaluate_ae(x, placeholder_x):\n",
    "    # TODO: evaluate your autoencoder\n",
    "    #tf.reset_default_graph()\n",
    "\n",
    "    placeholder_xf = convert_image_data_to_float(placeholder_x)\n",
    "    # TODO: implement autoencoder training\n",
    "    encoder_output = CAE_encoder(placeholder_xf, 'Encoder')\n",
    "    decoder_output = CAE_decoder(encoder_output, 'Decoder')\n",
    "    loss = tf.reduce_mean(tf.square(tf.subtract(decoder_output, placeholder_xf)))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, \"./ae_model/best/model.ckpt\")\n",
    "        print(\"Model restored.\")\n",
    "        \n",
    "        #output pixelwise loss using mse\n",
    "        l, fm, img = sess.run([loss, encoder_output, decoder_output], feed_dict={placeholder_x: x})\n",
    "        print(\"MSE pixelwise loss on test set: {}\".format(l))\n",
    "        \n",
    "        #random selecting 10 images\n",
    "        \n",
    "        for _ in range(10):\n",
    "            i = randint(0 , x.shape[0])\n",
    "            visualize_ae(i, x, fm, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-26-dcb457459fdd>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-dcb457459fdd>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    parser.add_argument('--task', default=\"train\", type=str,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='COMP5212 Programming Project 2')\n",
    "    parser.add_argument('--task', default=\"train\", type=str,\n",
    "                        help='Select the task, train_cnn, train_cnn_pretrained, test_cnn, test_cnn_pretrained, '\n",
    "                             'train_ae, evaluate_ae, ')\n",
    "    parser.add_argument('--p', default = \"0.5\",type = float, help = 'portion of training(0.5 or 1)')\n",
    "    parser.add_argument('--datapath',default=\"./data\",type=str, required=False,\n",
    "                        help='Select the path to the data directory')\n",
    "    args = parser.parse_args()\n",
    "    assert(args.p == 0.5 or args.p == 1)\n",
    "    p = args.p\n",
    "    datapath = args.datapath\n",
    "\n",
    "    with tf.variable_scope(\"placeholders\"):\n",
    "        img_var = tf.placeholder(tf.uint8, shape=(None, 28, 28), name=\"img\")\n",
    "        label_var = tf.placeholder(tf.int32, shape=(None,), name=\"true_label\")\n",
    "\n",
    "    if args.task == \"train_cnn\":\n",
    "        file_train = np.load(datapath+\"/data_classifier_train.npz\")\n",
    "        x_train = file_train[\"x_train\"]\n",
    "        y_train = file_train[\"y_train\"]\n",
    "        train_cnn(x_train, y_train, img_var, label_var)\n",
    "    elif args.task == \"test_cnn\":\n",
    "        file_test = np.load(datapath+\"/data_classifier_test.npz\")\n",
    "        x_test = file_test[\"x_test\"]\n",
    "        y_test = file_test[\"y_test\"]\n",
    "        accuracy = test_cnn(p,x_test, y_test,img_var,label_var)\n",
    "        #print(\"accuracy = {}\\n\".format(accuracy))\n",
    "    elif args.task == \"train_cnn_pretrained\":\n",
    "        file_train = np.load(datapath+\"/data_classifier_train.npz\")\n",
    "        x_train = file_train[\"x_train\"]\n",
    "        y_train = file_train[\"y_train\"]\n",
    "        train_cnn_pretrained(x_train, y_train, img_var, label_var)\n",
    "    elif args.task == \"test_cnn_pretrained\":\n",
    "        file_test = np.load(datapath+\"/data_classifier_test.npz\")\n",
    "        x_test = file_test[\"x_test\"]\n",
    "        y_test = file_test[\"y_test\"]\n",
    "        accuracy = test_cnn_pretrained(p,x_test, y_test,img_var,label_var)\n",
    "        #print(\"accuracy = {}\\n\".format(accuracy))\n",
    "    elif args.task == \"train_ae\":\n",
    "        file_unsupervised = np.load(datapath + \"/data_autoencoder_train.npz\")\n",
    "        x_ae_train = file_unsupervised[\"x_ae_train\"]\n",
    "        # x_ae_train = x_ae_train[:1000]\n",
    "        train_ae(x_ae_train, img_var)\n",
    "    elif args.task == \"evaluate_ae\":\n",
    "        file_unsupervised = np.load(datapath + \"/data_autoencoder_test.npz\")\n",
    "        x_ae_eval = file_unsupervised[\"x_ae_eval\"]\n",
    "        evaluate_ae(x_ae_eval, img_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
