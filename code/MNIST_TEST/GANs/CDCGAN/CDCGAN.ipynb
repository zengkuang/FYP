{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Deep Convolutional Generative Adversarial Network (CDCGAN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the MNIST dataset. input_data is a library that downloads the dataset and uzips it automatically. It can be acquired Github here: https://gist.github.com/awjuliani/1d21151bc17362bf6738c3dc02f37906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 50000\n",
    "IMG_TO_SHOW = 6\n",
    "IMG_CHN = 1\n",
    "TRAIN_SIZE = 4\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "G_LR = 1e-5\n",
    "D_LR = 1e-5\n",
    "ITER = 2500\n",
    "MEMORY_FACTOR = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MNISTDataloader:\n",
    "    def __init__(self, train_data_path, visualization = False):\n",
    "        train = np.load(train_data_path)\n",
    "        self.train_X, self.train_y = shuffle(train['x'][:TRAIN_SIZE], train['y'][:TRAIN_SIZE], random_state = 77)\n",
    "        self.test_X, self.test_y = shuffle(train['x'][TRAIN_SIZE:], train['y'][TRAIN_SIZE:], random_state = 77)\n",
    "        self.train_batch_ind = 0\n",
    "        self.test_batch_ind = 0\n",
    "        \n",
    "        if (visualization):\n",
    "            print(\"train X size: {}\".format(self.train_X.shape))\n",
    "            print(\"train y size: {}\".format(self.train_y.shape))\n",
    "            print(\"test X size: {}\".format(self.test_X.shape))\n",
    "            print(\"test y size: {}\".format(self.test_y.shape))\n",
    "            for i in [0, 1, 2]:\n",
    "                plt.matshow(train_X[i], cmap=plt.get_cmap('gray'))\n",
    "                plt.title(\"\" + str(i + 1) + \"th Training Data \")\n",
    "                plt.matshow(train_y[i], cmap=plt.get_cmap('gray'))\n",
    "                plt.title(\"\" + str(i + 1) + \"th Ground Truth \")\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size = 32, mode = \"train\"):\n",
    "        out = None\n",
    "        if mode == \"train\":\n",
    "            out= [self.train_X[batch_size* self.train_batch_ind : batch_size * (self.train_batch_ind+1)] , \n",
    "                     self.train_y[batch_size* self.train_batch_ind : batch_size * (self.train_batch_ind+1)]]\n",
    "            self.train_batch_ind += 1\n",
    "            if (self.train_batch_ind +1)* batch_size >= TRAIN_SIZE:\n",
    "                self.train_batch_ind = 0\n",
    "        elif mode == \"test\":\n",
    "            out= [self.test_X[batch_size* self.train_batch_ind : batch_size * (self.train_batch_ind+1)] , \n",
    "                     self.test_y[batch_size* self.train_batch_ind : batch_size * (self.train_batch_ind+1)]]\n",
    "            self.test_batch_ind += 1\n",
    "            if (self.test_batch_ind+1) * batch_size >= DATA_SIZE -TRAIN_SIZE:\n",
    "                self.test_batch_ind = 0\n",
    "        else:\n",
    "            raise(\"Mode error.\")\n",
    "        \n",
    "        return out\n",
    "    \n",
    "dl = MNISTDataloader(\"./mnist_inpainting.npz\", visualization = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "NUM_LABELS = 47\n",
    "rnd = np.random.RandomState(123)\n",
    "tf.set_random_seed(123)\n",
    "batch_size = 128\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img\n",
    "\n",
    "def upsample(input, name, factor=[2,2]):\n",
    "    size = [int(input.shape[1] * factor[0]), int(input.shape[2] * factor[1])]\n",
    "    with tf.name_scope(name):\n",
    "        out = tf.image.resize_nearest_neighbor(input, size=size, align_corners=False, name=None)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAE as image feature extractor for condition input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cae_encoder(x, reuse = False):\n",
    "    \n",
    "    c1 = slim.convolution2d(x, 32, [3,3], stride=[1,1], padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='conv1')\n",
    "    \n",
    "    p1 = slim.max_pool2d(c1, [2, 2], scope='pool1')\n",
    "    \n",
    "    c2 = slim.convolution2d(p1, 64, [3,3], stride=[2,2], padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='conv2')\n",
    "    \n",
    "    c3 = slim.convolution2d(c2, 64, [3,3], stride=[1,1], padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='conv3')\n",
    "\n",
    "    e_out = slim.max_pool2d(c3, [2, 2], scope='pool2')\n",
    "    \n",
    "    return e_out\n",
    "\n",
    "def cae_decoder(f, reuse = False):\n",
    "    \n",
    "    up1 = upsample(f, name = \"up1\")\n",
    "\n",
    "    dc_1 = slim.convolution2d_transpose(\\\n",
    "        up1,num_outputs=64, kernel_size=[3,3],stride=[1,1],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='dconv_1' )\n",
    "\n",
    "    dc_2 = slim.convolution2d_transpose(\\\n",
    "        dc_1, num_outputs=32, kernel_size=[3,3],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='dconv_2' )\n",
    "\n",
    "\n",
    "    up2 = upsample(dc_2, name = \"up2\")\n",
    "\n",
    "    dc_3 = slim.convolution2d_transpose(\\\n",
    "        up2, num_outputs=IMG_CHN, kernel_size=[3,3],stride=[1,1],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.sigmoid,scope='dconv_3' )\n",
    "\n",
    "    return dc_3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, c):\n",
    "    \n",
    "    with tf.variable_scope(\"gen\"):\n",
    "    \n",
    "        zcP = tf.concat([z, c],1)\n",
    "\n",
    "        g_in = slim.fully_connected(zcP,4*4*128,normalizer_fn= slim.batch_norm,\\\n",
    "            activation_fn=tf.nn.relu,scope='g_project')\n",
    "\n",
    "        g_in_Con = tf.reshape(g_in,[-1,4,4,128])\n",
    "\n",
    "        gen1 = slim.convolution2d_transpose(\\\n",
    "            g_in_Con,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
    "            padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "            activation_fn=tf.nn.relu,scope='g_conv1')\n",
    "\n",
    "        gen2 = slim.convolution2d_transpose(\\\n",
    "            gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
    "            padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "            activation_fn=tf.nn.relu,scope='g_conv2')\n",
    "\n",
    "        gen3 = slim.convolution2d_transpose(\\\n",
    "            gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
    "            padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "            activation_fn=tf.nn.relu,scope='g_conv3')\n",
    "\n",
    "        g_out = slim.convolution2d_transpose(\\\n",
    "            gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "            biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "            scope='g_out')\n",
    "\n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(img, c, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"dis\"):\n",
    "\n",
    "        dis1 = slim.convolution2d(img,16,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "            biases_initializer=None,activation_fn=lrelu,\\\n",
    "            reuse=reuse,scope='d_conv1')\n",
    "\n",
    "        dis2 = slim.convolution2d(dis1,32,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "            normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "            reuse=reuse,scope='d_conv2')\n",
    "\n",
    "        dis3 = slim.convolution2d(dis2,32,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "            normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "            reuse=reuse,scope='d_conv3')\n",
    "\n",
    "        d_fc1 = slim.fully_connected(slim.flatten(dis1),512,activation_fn=tf.nn.relu,\\\n",
    "            reuse=reuse,scope='d_fc1')\n",
    "\n",
    "        d_combined =  tf.concat([d_fc1, c],1)\n",
    "\n",
    "\n",
    "        d_fc2 = slim.fully_connected(slim.flatten(d_combined),128,activation_fn=tf.nn.relu,\\\n",
    "            reuse=reuse,scope='d_fc2')\n",
    "\n",
    "        d_out= slim.fully_connected(slim.flatten(d_combined),1,activation_fn=tf.nn.sigmoid,\\\n",
    "            reuse=reuse,scope='d_out')\n",
    "\n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../CAE/models\\model-4000.cptk\n",
      "CAE restored.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "c_in = tf.placeholder(shape = [None,32,32,IMG_CHN],dtype = tf.float32) # conditional input\n",
    "real_in = tf.placeholder(shape=[None,32,32,IMG_CHN],dtype=tf.float32) #Real images\n",
    "\n",
    "'''\n",
    "Define GAN graph\n",
    "'''\n",
    "fx = cae_encoder(c_in)\n",
    "model_directory = '../../CAE/models' #Directory to load trained model from.\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    #Reload CAE the model\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    print(\"CAE restored.\")\n",
    "    \n",
    "c_in_flat = tf.layers.Flatten()(fx)\n",
    "Gz = generator(z_in, c_in_flat) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in, c_in_flat) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz,c_in_flat,reuse=True) #Produces probabilities for generator images\n",
    "pixel_wise_diff = (Gz - real_in)\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "gp_loss = tf.reduce_mean((Gz - real_in)**2)\n",
    "\n",
    "dtvars = tf.trainable_variables(scope = \"dis\")\n",
    "gtvars = tf.trainable_variables(scope = \"gen\")\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=D_LR,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=G_LR,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,dtvars) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss + MEMORY_FACTOR*gp_loss, gtvars) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 58.274704 Disc Loss: 1.3864276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 47.414364 Disc Loss: 1.2976454\n",
      "Gen Loss: 41.445053 Disc Loss: 1.1934519\n",
      "Gen Loss: 35.834747 Disc Loss: 1.1710944\n",
      "Gen Loss: 31.663208 Disc Loss: 1.1095582\n",
      "Gen Loss: 29.250317 Disc Loss: 1.0621705\n",
      "Gen Loss: 26.412277 Disc Loss: 1.0303127\n",
      "Gen Loss: 24.301678 Disc Loss: 0.9928082\n",
      "Gen Loss: 23.171516 Disc Loss: 0.955642\n",
      "Gen Loss: 20.519938 Disc Loss: 0.92453855\n",
      "Gen Loss: 20.14537 Disc Loss: 0.8881037\n",
      "Gen Loss: 18.732708 Disc Loss: 0.85688305\n",
      "Gen Loss: 18.446308 Disc Loss: 0.8317814\n",
      "Gen Loss: 17.47867 Disc Loss: 0.8032989\n",
      "Gen Loss: 16.685642 Disc Loss: 0.78876114\n",
      "Gen Loss: 16.932745 Disc Loss: 0.75745714\n",
      "Gen Loss: 15.757004 Disc Loss: 0.7475743\n",
      "Gen Loss: 14.906557 Disc Loss: 0.71972936\n",
      "Gen Loss: 14.573246 Disc Loss: 0.7251408\n",
      "Gen Loss: 14.180852 Disc Loss: 0.7125781\n",
      "Gen Loss: 13.522078 Disc Loss: 0.68323576\n",
      "Gen Loss: 12.787437 Disc Loss: 0.69956726\n",
      "Gen Loss: 13.031471 Disc Loss: 0.6744321\n",
      "Gen Loss: 11.871908 Disc Loss: 0.674627\n",
      "Gen Loss: 12.007313 Disc Loss: 0.6737809\n",
      "Gen Loss: 11.510657 Disc Loss: 0.68853104\n",
      "Gen Loss: 10.884295 Disc Loss: 0.6826549\n",
      "Gen Loss: 10.816678 Disc Loss: 0.6751013\n",
      "Gen Loss: 10.349506 Disc Loss: 0.6816333\n",
      "Gen Loss: 9.89929 Disc Loss: 0.7048179\n",
      "Gen Loss: 9.739079 Disc Loss: 0.70331585\n",
      "Gen Loss: 9.915016 Disc Loss: 0.6823724\n",
      "Gen Loss: 9.597464 Disc Loss: 0.70237404\n",
      "Gen Loss: 9.121231 Disc Loss: 0.72562987\n",
      "Gen Loss: 9.11158 Disc Loss: 0.7182494\n",
      "Gen Loss: 9.000859 Disc Loss: 0.7233034\n",
      "Gen Loss: 8.444679 Disc Loss: 0.7348485\n",
      "Gen Loss: 8.2847 Disc Loss: 0.7336085\n",
      "Gen Loss: 8.506815 Disc Loss: 0.7277128\n",
      "Gen Loss: 8.430901 Disc Loss: 0.7508557\n",
      "Gen Loss: 8.233981 Disc Loss: 0.76350826\n",
      "Gen Loss: 7.6995225 Disc Loss: 0.75259143\n",
      "Gen Loss: 7.6121397 Disc Loss: 0.7679856\n",
      "Gen Loss: 7.812439 Disc Loss: 0.739037\n",
      "Gen Loss: 7.7539186 Disc Loss: 0.753222\n",
      "Gen Loss: 7.5181956 Disc Loss: 0.7661749\n",
      "Gen Loss: 7.4852767 Disc Loss: 0.7525721\n",
      "Gen Loss: 7.1042953 Disc Loss: 0.7606711\n",
      "Gen Loss: 6.8875837 Disc Loss: 0.76599586\n",
      "Gen Loss: 6.880976 Disc Loss: 0.76395917\n",
      "Gen Loss: 6.8940544 Disc Loss: 0.75791883\n",
      "Gen Loss: 7.025811 Disc Loss: 0.74793136\n",
      "Gen Loss: 6.883692 Disc Loss: 0.77095366\n",
      "Gen Loss: 6.856632 Disc Loss: 0.7494931\n",
      "Gen Loss: 6.985133 Disc Loss: 0.76424587\n",
      "Gen Loss: 6.676684 Disc Loss: 0.7739817\n",
      "Gen Loss: 6.5686913 Disc Loss: 0.77646995\n",
      "Gen Loss: 6.3314033 Disc Loss: 0.7838578\n",
      "Gen Loss: 6.428168 Disc Loss: 0.7536785\n",
      "Gen Loss: 6.484665 Disc Loss: 0.7498632\n",
      "Gen Loss: 6.333034 Disc Loss: 0.7479746\n",
      "Gen Loss: 6.206194 Disc Loss: 0.7512019\n",
      "Gen Loss: 6.3222075 Disc Loss: 0.76960164\n",
      "Gen Loss: 6.208858 Disc Loss: 0.7505899\n",
      "Gen Loss: 6.3602934 Disc Loss: 0.758278\n",
      "Gen Loss: 6.235174 Disc Loss: 0.75036913\n",
      "Gen Loss: 6.355649 Disc Loss: 0.74025327\n",
      "Gen Loss: 6.085984 Disc Loss: 0.76418114\n",
      "Gen Loss: 6.1225395 Disc Loss: 0.7573836\n",
      "Gen Loss: 6.193346 Disc Loss: 0.75762224\n",
      "Gen Loss: 5.842757 Disc Loss: 0.7602502\n",
      "Gen Loss: 6.1091614 Disc Loss: 0.7472599\n",
      "Gen Loss: 5.991292 Disc Loss: 0.7407825\n",
      "Gen Loss: 6.303923 Disc Loss: 0.7184832\n",
      "Gen Loss: 5.886447 Disc Loss: 0.7383848\n",
      "Gen Loss: 5.775897 Disc Loss: 0.75986737\n",
      "Gen Loss: 5.767801 Disc Loss: 0.7698098\n",
      "Gen Loss: 5.621963 Disc Loss: 0.7542038\n",
      "Gen Loss: 5.9041886 Disc Loss: 0.7155633\n",
      "Gen Loss: 5.5990157 Disc Loss: 0.73432475\n",
      "Gen Loss: 5.732545 Disc Loss: 0.7267451\n",
      "Gen Loss: 5.768773 Disc Loss: 0.7272898\n",
      "Gen Loss: 5.4477615 Disc Loss: 0.7155285\n",
      "Gen Loss: 5.678133 Disc Loss: 0.7303438\n",
      "Gen Loss: 5.587472 Disc Loss: 0.7488948\n",
      "Gen Loss: 5.6171255 Disc Loss: 0.7354355\n",
      "Gen Loss: 5.502225 Disc Loss: 0.72448444\n",
      "Gen Loss: 5.4627714 Disc Loss: 0.72752994\n",
      "Gen Loss: 5.720633 Disc Loss: 0.71113056\n",
      "Gen Loss: 5.5006003 Disc Loss: 0.70687807\n",
      "Gen Loss: 5.441839 Disc Loss: 0.7071887\n",
      "Gen Loss: 5.5718174 Disc Loss: 0.6983781\n",
      "Gen Loss: 5.338125 Disc Loss: 0.71893156\n",
      "Gen Loss: 5.3756533 Disc Loss: 0.70511866\n",
      "Gen Loss: 5.087661 Disc Loss: 0.73572004\n",
      "Gen Loss: 5.2833757 Disc Loss: 0.71766025\n",
      "Gen Loss: 5.2093353 Disc Loss: 0.7131032\n",
      "Gen Loss: 5.4350114 Disc Loss: 0.6868612\n",
      "Gen Loss: 5.2632837 Disc Loss: 0.7002743\n"
     ]
    }
   ],
   "source": [
    "batch_size = TRAIN_BATCH_SIZE #Size of image batch to apply at each iteration.\n",
    "iterations = ITER #Total number of iterations to use.\n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "GAN_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"gen\") + tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"dis\")\n",
    "gan_init = tf.variables_initializer(GAN_list)\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(gan_init)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        [xs, ys]= dl.next_batch(batch_size = batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = np.reshape(xs,[batch_size,28,28,IMG_CHN])  #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        ys = np.reshape(ys,[batch_size,28,28,IMG_CHN])  #Transform it to be between -1 and 1\n",
    "        ys = np.lib.pad(ys, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        \n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs, real_in:ys, c_in:xs }) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G, g_loss + MEMORY_FACTOR*gp_loss],feed_dict={z_in:zs, real_in:ys, c_in:xs}) #Update the generator, twice for good measure.\n",
    "        _,gLoss = sess.run([update_G, g_loss + MEMORY_FACTOR*gp_loss],feed_dict={z_in:zs, real_in:ys, c_in:xs})\n",
    "        \n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2, c_in:xs}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(np.reshape(newZ[0:IMG_TO_SHOW**2],[IMG_TO_SHOW**2,32,32]),[IMG_TO_SHOW,IMG_TO_SHOW],\n",
    "                        sample_directory+'/fig'+str(i)+'.png') #change to 36\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "batch_size_sample = 36\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print 'Loading Model...'\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    zs = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32) #Generate a random z batch\n",
    "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
