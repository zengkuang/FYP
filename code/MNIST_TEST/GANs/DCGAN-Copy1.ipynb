{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Network (DCGAN) Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorials walks through an implementation of DCGAN as described in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "To learn more about generative adversarial networks, see my [Medium post](https://medium.com/p/54deab2fce39) on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the MNIST dataset. input_data is a library that downloads the dataset and uzips it automatically. It can be acquired Github here: https://gist.github.com/awjuliani/1d21151bc17362bf6738c3dc02f37906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28)\n",
      "(55000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print out several data in mnist\n",
    "trainimg = mnist.train.images\n",
    "train_y = mnist.train.labels\n",
    "nsample = 1\n",
    "randidx = np.random.randint(trainimg.shape[0], size=nsample)\n",
    "\n",
    "train_x = np.reshape(trainimg,(trainimg.shape[0],28,28))\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    curr_img   = np.reshape(trainimg[i, :], (28, 28)) # 28 by 28 matrix \n",
    "    curr_label = np.argmax(train_y[i] ) # Label\n",
    "    plt.matshow(curr_img, cmap=plt.get_cmap('gray'))\n",
    "    plt.title(\"\" + str(i + 1) + \"th Training Data \" \n",
    "              + \"Label is \" + str(curr_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "         f1 = 0.5 * (1 + leak)\n",
    "         f2 = 0.5 * (1 - leak)\n",
    "         return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, c):\n",
    "    \n",
    "    c_flat = tf.layers.Flatten()(c)\n",
    "    zcP = tf.concat([z, c_flat],1)\n",
    "    \n",
    "    \n",
    "    g_in = slim.fully_connected(zcP,4*4*256,normalizer_fn= slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    \n",
    "    g_in_Con = tf.reshape(g_in,[-1,4,4,256])\n",
    "    \n",
    "    \n",
    "    gen1 = slim.convolution2d_transpose(\\\n",
    "        g_in_Con,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    \n",
    "    gen2 = slim.convolution2d_transpose(\\\n",
    "        gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    gen3 = slim.convolution2d_transpose(\\\n",
    "        gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    \n",
    "    g_out = slim.convolution2d_transpose(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    print(g_out.shape)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n",
    "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(img, c, reuse=False):\n",
    "    \n",
    "    print(img.shape)\n",
    "    print(c.shape)\n",
    "    print(\"//\")\n",
    "    dis_img_1 = slim.convolution2d(img,16,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
    "    dis_c_1 = slim.convolution2d(c,16,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_c_conv1',weights_initializer=initializer)\n",
    "    '''\n",
    "    dis2 = slim.convolution2d(dis1,32,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    dis3 = slim.convolution2d(dis2,64,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
    "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
    "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
    "    '''\n",
    "    d_img_out = slim.fully_connected(slim.flatten(dis_img_1),64,activation_fn=tf.nn.relu,\\\n",
    "        reuse=reuse,scope='d_img_out', weights_initializer=initializer)\n",
    "    d_c_out = slim.fully_connected(slim.flatten(dis_c_1),64,activation_fn=tf.nn.relu,\\\n",
    "        reuse=reuse,scope='d_c_out', weights_initializer=initializer)\n",
    "    \n",
    "    fc1 = tf.concat([d_img_out, d_c_out],1)\n",
    "    d_out =  slim.fully_connected(slim.flatten(fc1),1,activation_fn=tf.nn.sigmoid,\\\n",
    "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
    "\n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 1)\n",
      "(?, 32, 32, 1)\n",
      "(?, 32, 32, 1)\n",
      "//\n",
      "(?, 32, 32, 1)\n",
      "(?, 32, 32, 1)\n",
      "//\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "c_in = tf.placeholder(shape = [None,32,32,1],dtype = tf.float32) # conditional input\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "Gz = generator(z_in, c_in) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in, c_in) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz,c_in,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "Now that we have fully defined our network, it is time to train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.68830794 Disc Loss: 1.3891736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.69389987 Disc Loss: 1.3865371\n",
      "Gen Loss: 0.6907045 Disc Loss: 1.3897834\n",
      "Gen Loss: 0.6940279 Disc Loss: 1.3853979\n",
      "Gen Loss: 0.691639 Disc Loss: 1.3910112\n",
      "Gen Loss: 0.69688416 Disc Loss: 1.380646\n",
      "Gen Loss: 0.7089751 Disc Loss: 1.3552366\n",
      "Gen Loss: 0.77050304 Disc Loss: 1.2604753\n",
      "Gen Loss: 0.43596888 Disc Loss: 1.8995246\n",
      "Gen Loss: 0.69646144 Disc Loss: 1.4107133\n",
      "Gen Loss: 0.70298356 Disc Loss: 1.3791263\n",
      "Gen Loss: 0.70265317 Disc Loss: 1.3955126\n",
      "Gen Loss: 0.6456228 Disc Loss: 1.35392\n",
      "Gen Loss: 0.6911453 Disc Loss: 1.4032626\n",
      "Gen Loss: 0.6945557 Disc Loss: 1.3932977\n",
      "Gen Loss: 0.70031005 Disc Loss: 1.3763602\n",
      "Gen Loss: 0.6931244 Disc Loss: 1.3927217\n",
      "Gen Loss: 0.6925979 Disc Loss: 1.392101\n",
      "Gen Loss: 0.6887018 Disc Loss: 1.4027785\n",
      "Gen Loss: 0.6977637 Disc Loss: 1.3843098\n",
      "Gen Loss: 0.6963022 Disc Loss: 1.3825601\n",
      "Gen Loss: 0.6925084 Disc Loss: 1.4019432\n",
      "Gen Loss: 0.6982346 Disc Loss: 1.386613\n",
      "Gen Loss: 0.7001039 Disc Loss: 1.393843\n",
      "Gen Loss: 0.6851486 Disc Loss: 1.412164\n",
      "Gen Loss: 0.69235873 Disc Loss: 1.3926758\n",
      "Gen Loss: 0.68977547 Disc Loss: 1.3963408\n",
      "Gen Loss: 0.6929103 Disc Loss: 1.3911382\n",
      "Gen Loss: 0.6883946 Disc Loss: 1.4001825\n",
      "Gen Loss: 0.6951005 Disc Loss: 1.3820235\n",
      "Gen Loss: 0.69026685 Disc Loss: 1.3787037\n",
      "Gen Loss: 0.6868999 Disc Loss: 1.4075329\n",
      "Gen Loss: 0.69467455 Disc Loss: 1.3892552\n",
      "Gen Loss: 0.69059527 Disc Loss: 1.3890407\n",
      "Gen Loss: 0.6917865 Disc Loss: 1.4014381\n",
      "Gen Loss: 0.6932498 Disc Loss: 1.3914866\n",
      "Gen Loss: 0.6962065 Disc Loss: 1.3885498\n",
      "Gen Loss: 0.6931031 Disc Loss: 1.3898537\n",
      "Gen Loss: 0.6905828 Disc Loss: 1.3956444\n",
      "Gen Loss: 0.6949917 Disc Loss: 1.3881493\n",
      "Gen Loss: 0.6891104 Disc Loss: 1.3963443\n",
      "Gen Loss: 0.6919583 Disc Loss: 1.3932352\n",
      "Gen Loss: 0.6888015 Disc Loss: 1.3960619\n",
      "Gen Loss: 0.6957275 Disc Loss: 1.3856773\n",
      "Gen Loss: 0.6940011 Disc Loss: 1.3879584\n",
      "Gen Loss: 0.6955466 Disc Loss: 1.385149\n",
      "Gen Loss: 0.6899904 Disc Loss: 1.3933747\n",
      "Gen Loss: 0.68714076 Disc Loss: 1.4025301\n",
      "Gen Loss: 0.69660515 Disc Loss: 1.3823209\n",
      "Gen Loss: 0.6918902 Disc Loss: 1.3910062\n",
      "Gen Loss: 0.69002724 Disc Loss: 1.3949983\n",
      "Gen Loss: 0.69523096 Disc Loss: 1.383565\n",
      "Gen Loss: 0.69177526 Disc Loss: 1.3908788\n",
      "Gen Loss: 0.6952871 Disc Loss: 1.3837006\n",
      "Gen Loss: 0.6914921 Disc Loss: 1.3934511\n",
      "Gen Loss: 0.69336224 Disc Loss: 1.3858659\n",
      "Gen Loss: 0.69169676 Disc Loss: 1.3925567\n",
      "Gen Loss: 0.6937197 Disc Loss: 1.3878303\n",
      "Gen Loss: 0.69445443 Disc Loss: 1.385204\n",
      "Gen Loss: 0.6926881 Disc Loss: 1.3890744\n",
      "Gen Loss: 0.6912372 Disc Loss: 1.3929498\n",
      "Gen Loss: 0.690946 Disc Loss: 1.3911933\n",
      "Gen Loss: 0.6938047 Disc Loss: 1.3872554\n",
      "Gen Loss: 0.6998728 Disc Loss: 1.3755884\n",
      "Gen Loss: 0.6928724 Disc Loss: 1.3716049\n",
      "Gen Loss: 0.6902801 Disc Loss: 1.3943723\n",
      "Gen Loss: 0.6920871 Disc Loss: 1.3899263\n",
      "Gen Loss: 0.69449234 Disc Loss: 1.3904912\n",
      "Gen Loss: 0.6950544 Disc Loss: 1.3844182\n",
      "Gen Loss: 0.6931431 Disc Loss: 1.3880217\n",
      "Gen Loss: 0.692971 Disc Loss: 1.3850976\n",
      "Gen Loss: 0.69429755 Disc Loss: 1.3857169\n",
      "Gen Loss: 0.69431365 Disc Loss: 1.3927224\n",
      "Gen Loss: 0.6972952 Disc Loss: 1.3804383\n",
      "Gen Loss: 0.6894791 Disc Loss: 1.3951283\n",
      "Gen Loss: 0.69359857 Disc Loss: 1.3903077\n",
      "Gen Loss: 0.69278365 Disc Loss: 1.3902879\n",
      "Gen Loss: 0.69152284 Disc Loss: 1.3909593\n",
      "Gen Loss: 0.69343567 Disc Loss: 1.3886214\n",
      "Gen Loss: 0.69671416 Disc Loss: 1.3813437\n",
      "Gen Loss: 0.69341624 Disc Loss: 1.3906941\n",
      "Gen Loss: 0.68829644 Disc Loss: 1.4036341\n",
      "Gen Loss: 0.69420254 Disc Loss: 1.3902066\n",
      "Gen Loss: 0.6944319 Disc Loss: 1.3812308\n",
      "Gen Loss: 0.6926146 Disc Loss: 1.391577\n",
      "Gen Loss: 0.6961547 Disc Loss: 1.3856559\n",
      "Gen Loss: 0.693308 Disc Loss: 1.3898551\n",
      "Gen Loss: 0.693325 Disc Loss: 1.3888268\n",
      "Gen Loss: 0.6963811 Disc Loss: 1.3818529\n",
      "Gen Loss: 0.69279975 Disc Loss: 1.3883069\n",
      "Gen Loss: 0.69494355 Disc Loss: 1.3834558\n",
      "Gen Loss: 0.6934675 Disc Loss: 1.388284\n",
      "Gen Loss: 0.69523907 Disc Loss: 1.3841754\n",
      "Gen Loss: 0.69525313 Disc Loss: 1.3863267\n",
      "Gen Loss: 0.69163615 Disc Loss: 1.3925141\n",
      "Gen Loss: 0.6940086 Disc Loss: 1.386368\n",
      "Gen Loss: 0.692266 Disc Loss: 1.388552\n",
      "Gen Loss: 0.69229746 Disc Loss: 1.3887835\n",
      "Gen Loss: 0.6917646 Disc Loss: 1.3904473\n",
      "Gen Loss: 0.6943477 Disc Loss: 1.3853378\n",
      "Gen Loss: 0.69598943 Disc Loss: 1.3814988\n",
      "Saved Model\n",
      "Gen Loss: 0.69872797 Disc Loss: 1.3822365\n",
      "Gen Loss: 0.69985425 Disc Loss: 1.3811808\n",
      "Gen Loss: 0.6861293 Disc Loss: 1.4030979\n",
      "Gen Loss: 0.6925275 Disc Loss: 1.3810024\n",
      "Gen Loss: 0.6956124 Disc Loss: 1.3972039\n",
      "Gen Loss: 0.6911528 Disc Loss: 1.3981318\n",
      "Gen Loss: 0.70052016 Disc Loss: 1.375993\n",
      "Gen Loss: 0.6889688 Disc Loss: 1.3963654\n",
      "Gen Loss: 0.695792 Disc Loss: 1.3843296\n",
      "Gen Loss: 0.693705 Disc Loss: 1.391048\n",
      "Gen Loss: 0.68857944 Disc Loss: 1.390024\n",
      "Gen Loss: 0.6927998 Disc Loss: 1.3908998\n",
      "Gen Loss: 0.69202244 Disc Loss: 1.3910956\n",
      "Gen Loss: 0.6931414 Disc Loss: 1.3879076\n",
      "Gen Loss: 0.6930966 Disc Loss: 1.3846347\n",
      "Gen Loss: 0.7027614 Disc Loss: 1.3777628\n",
      "Gen Loss: 0.69716424 Disc Loss: 1.4023967\n",
      "Gen Loss: 0.6983061 Disc Loss: 1.3801765\n",
      "Gen Loss: 0.7176211 Disc Loss: 1.35108\n",
      "Gen Loss: 0.6917847 Disc Loss: 1.4184096\n",
      "Gen Loss: 0.69277936 Disc Loss: 1.3929186\n",
      "Gen Loss: 0.69862646 Disc Loss: 1.3538163\n",
      "Gen Loss: 0.68260086 Disc Loss: 1.3965281\n",
      "Gen Loss: 0.698496 Disc Loss: 1.3828406\n",
      "Gen Loss: 0.6887834 Disc Loss: 1.3978317\n",
      "Gen Loss: 0.6934677 Disc Loss: 1.3898083\n",
      "Gen Loss: 0.69725835 Disc Loss: 1.3797717\n",
      "Gen Loss: 0.69017917 Disc Loss: 1.398787\n",
      "Gen Loss: 0.6923599 Disc Loss: 1.3894258\n",
      "Gen Loss: 0.69361866 Disc Loss: 1.3879521\n",
      "Gen Loss: 0.69187963 Disc Loss: 1.3916569\n",
      "Gen Loss: 0.6944206 Disc Loss: 1.385288\n",
      "Gen Loss: 0.6935626 Disc Loss: 1.3882455\n",
      "Gen Loss: 0.6947397 Disc Loss: 1.3837821\n",
      "Gen Loss: 0.69561875 Disc Loss: 1.3850561\n",
      "Gen Loss: 0.6889753 Disc Loss: 1.3971711\n",
      "Gen Loss: 0.6905155 Disc Loss: 1.3943\n",
      "Gen Loss: 0.69452083 Disc Loss: 1.3853731\n",
      "Gen Loss: 0.69730073 Disc Loss: 1.387687\n",
      "Gen Loss: 0.69596076 Disc Loss: 1.3921347\n",
      "Gen Loss: 0.6988441 Disc Loss: 1.3749464\n",
      "Gen Loss: 0.68391633 Disc Loss: 1.4078473\n",
      "Gen Loss: 0.69390225 Disc Loss: 1.3828199\n",
      "Gen Loss: 0.69349045 Disc Loss: 1.3869641\n",
      "Gen Loss: 0.69194174 Disc Loss: 1.3898548\n",
      "Gen Loss: 0.69333845 Disc Loss: 1.3874972\n",
      "Gen Loss: 0.69410145 Disc Loss: 1.3888915\n",
      "Gen Loss: 0.6951339 Disc Loss: 1.3855\n",
      "Gen Loss: 0.68786883 Disc Loss: 1.3999777\n",
      "Gen Loss: 0.68931824 Disc Loss: 1.3957069\n",
      "Gen Loss: 0.693385 Disc Loss: 1.3868082\n",
      "Gen Loss: 0.6924526 Disc Loss: 1.3890746\n",
      "Gen Loss: 0.69453806 Disc Loss: 1.3853471\n",
      "Gen Loss: 0.6941282 Disc Loss: 1.3867975\n",
      "Gen Loss: 0.6974756 Disc Loss: 1.3791196\n",
      "Gen Loss: 0.6918188 Disc Loss: 1.39674\n",
      "Gen Loss: 0.6899263 Disc Loss: 1.3924031\n",
      "Gen Loss: 0.6935537 Disc Loss: 1.3884544\n",
      "Gen Loss: 0.6996381 Disc Loss: 1.3764746\n",
      "Gen Loss: 0.68781567 Disc Loss: 1.3974497\n",
      "Gen Loss: 0.6923441 Disc Loss: 1.3901279\n",
      "Gen Loss: 0.69412214 Disc Loss: 1.3848248\n",
      "Gen Loss: 0.6957469 Disc Loss: 1.382284\n",
      "Gen Loss: 0.6917561 Disc Loss: 1.3951693\n",
      "Gen Loss: 0.69233084 Disc Loss: 1.3889885\n",
      "Gen Loss: 0.69459116 Disc Loss: 1.3863966\n",
      "Gen Loss: 0.69339585 Disc Loss: 1.3885095\n",
      "Gen Loss: 0.6987699 Disc Loss: 1.3795671\n",
      "Gen Loss: 0.6934824 Disc Loss: 1.3950393\n",
      "Gen Loss: 0.69303095 Disc Loss: 1.3934866\n",
      "Gen Loss: 0.6940254 Disc Loss: 1.3861344\n",
      "Gen Loss: 0.6954545 Disc Loss: 1.3838544\n",
      "Gen Loss: 0.6893212 Disc Loss: 1.3935575\n",
      "Gen Loss: 0.6930617 Disc Loss: 1.3880823\n",
      "Gen Loss: 0.6947699 Disc Loss: 1.385385\n",
      "Gen Loss: 0.69308996 Disc Loss: 1.3871238\n",
      "Gen Loss: 0.6930131 Disc Loss: 1.3867699\n",
      "Gen Loss: 0.69035196 Disc Loss: 1.3925732\n",
      "Gen Loss: 0.6935491 Disc Loss: 1.3865968\n",
      "Gen Loss: 0.695675 Disc Loss: 1.3828733\n",
      "Gen Loss: 0.6886158 Disc Loss: 1.3961257\n",
      "Gen Loss: 0.6923259 Disc Loss: 1.3884233\n",
      "Gen Loss: 0.69430983 Disc Loss: 1.384539\n",
      "Gen Loss: 0.69807637 Disc Loss: 1.3827987\n",
      "Gen Loss: 0.6996591 Disc Loss: 1.3756986\n",
      "Gen Loss: 0.67658365 Disc Loss: 1.3963101\n",
      "Gen Loss: 0.69453424 Disc Loss: 1.3901446\n",
      "Gen Loss: 0.69216657 Disc Loss: 1.392375\n",
      "Gen Loss: 0.69332945 Disc Loss: 1.3875701\n",
      "Gen Loss: 0.69471335 Disc Loss: 1.3843802\n",
      "Gen Loss: 0.6963942 Disc Loss: 1.3820235\n",
      "Gen Loss: 0.6933515 Disc Loss: 1.3875887\n",
      "Gen Loss: 0.69218206 Disc Loss: 1.3901153\n",
      "Gen Loss: 0.6910877 Disc Loss: 1.3921893\n",
      "Gen Loss: 0.6943151 Disc Loss: 1.3856821\n",
      "Gen Loss: 0.6957021 Disc Loss: 1.3825015\n",
      "Gen Loss: 0.6948802 Disc Loss: 1.3860874\n",
      "Gen Loss: 0.6929585 Disc Loss: 1.3838171\n",
      "Gen Loss: 0.69070077 Disc Loss: 1.3922853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.69137144 Disc Loss: 1.3916886\n",
      "Saved Model\n",
      "Gen Loss: 0.69422853 Disc Loss: 1.3858912\n",
      "Gen Loss: 0.69651246 Disc Loss: 1.3809104\n",
      "Gen Loss: 0.69843084 Disc Loss: 1.3763251\n",
      "Gen Loss: 0.69460654 Disc Loss: 1.3946321\n",
      "Gen Loss: 0.69973266 Disc Loss: 1.3748715\n",
      "Gen Loss: 0.69688284 Disc Loss: 1.3911616\n",
      "Gen Loss: 0.6971364 Disc Loss: 1.3818791\n",
      "Gen Loss: 0.69122154 Disc Loss: 1.3905009\n",
      "Gen Loss: 0.69416296 Disc Loss: 1.3834028\n",
      "Gen Loss: 0.6908132 Disc Loss: 1.3945336\n",
      "Gen Loss: 0.6931323 Disc Loss: 1.3892963\n",
      "Gen Loss: 0.69371355 Disc Loss: 1.3860514\n",
      "Gen Loss: 0.6921449 Disc Loss: 1.3913442\n",
      "Gen Loss: 0.6908519 Disc Loss: 1.3952012\n",
      "Gen Loss: 0.69420964 Disc Loss: 1.3856604\n",
      "Gen Loss: 0.6975477 Disc Loss: 1.3792555\n",
      "Gen Loss: 0.70250607 Disc Loss: 1.3679837\n",
      "Gen Loss: 0.68771404 Disc Loss: 1.4057934\n",
      "Gen Loss: 0.6965061 Disc Loss: 1.3822758\n",
      "Gen Loss: 0.7018073 Disc Loss: 1.3712354\n",
      "Gen Loss: 0.68857116 Disc Loss: 1.4041699\n",
      "Gen Loss: 0.6931025 Disc Loss: 1.389252\n",
      "Gen Loss: 0.6989893 Disc Loss: 1.373024\n",
      "Gen Loss: 0.6991263 Disc Loss: 1.3992963\n",
      "Gen Loss: 0.692483 Disc Loss: 1.3884017\n",
      "Gen Loss: 0.6902075 Disc Loss: 1.39673\n",
      "Gen Loss: 0.6921668 Disc Loss: 1.3899052\n",
      "Gen Loss: 0.69649494 Disc Loss: 1.3824171\n",
      "Gen Loss: 0.69389963 Disc Loss: 1.3911946\n",
      "Gen Loss: 0.69134676 Disc Loss: 1.391598\n",
      "Gen Loss: 0.69236314 Disc Loss: 1.3908138\n",
      "Gen Loss: 0.69215655 Disc Loss: 1.3904438\n",
      "Gen Loss: 0.69434536 Disc Loss: 1.38645\n",
      "Gen Loss: 0.69194114 Disc Loss: 1.391994\n",
      "Gen Loss: 0.69443893 Disc Loss: 1.3851252\n",
      "Gen Loss: 0.6982994 Disc Loss: 1.3768466\n",
      "Gen Loss: 0.6889218 Disc Loss: 1.4016417\n",
      "Gen Loss: 0.6924233 Disc Loss: 1.3867872\n",
      "Gen Loss: 0.69508135 Disc Loss: 1.3828292\n",
      "Gen Loss: 0.69203174 Disc Loss: 1.3918929\n",
      "Gen Loss: 0.69408685 Disc Loss: 1.382419\n",
      "Gen Loss: 0.6967457 Disc Loss: 1.3788784\n",
      "Gen Loss: 0.6971663 Disc Loss: 1.3845248\n",
      "Gen Loss: 0.6927962 Disc Loss: 1.3943598\n",
      "Gen Loss: 0.69910526 Disc Loss: 1.3756874\n",
      "Gen Loss: 0.69726944 Disc Loss: 1.3616444\n",
      "Gen Loss: 0.6811632 Disc Loss: 1.4218183\n",
      "Gen Loss: 0.69361895 Disc Loss: 1.3880122\n",
      "Gen Loss: 0.6973657 Disc Loss: 1.3861592\n",
      "Gen Loss: 0.69398546 Disc Loss: 1.397449\n",
      "Gen Loss: 0.6920905 Disc Loss: 1.3895754\n",
      "Gen Loss: 0.7010591 Disc Loss: 1.3722358\n",
      "Gen Loss: 0.68926907 Disc Loss: 1.4000382\n",
      "Gen Loss: 0.69672084 Disc Loss: 1.3823969\n",
      "Gen Loss: 0.6922782 Disc Loss: 1.3866651\n",
      "Gen Loss: 0.6924565 Disc Loss: 1.4107496\n",
      "Gen Loss: 0.69385874 Disc Loss: 1.392267\n",
      "Gen Loss: 0.7085035 Disc Loss: 1.366623\n",
      "Gen Loss: 0.70539236 Disc Loss: 1.3821126\n",
      "Gen Loss: 0.6936482 Disc Loss: 1.389373\n",
      "Gen Loss: 0.6969064 Disc Loss: 1.3794379\n",
      "Gen Loss: 0.7014576 Disc Loss: 1.4002459\n",
      "Gen Loss: 0.69530725 Disc Loss: 1.3931668\n",
      "Gen Loss: 0.6972425 Disc Loss: 1.3822527\n",
      "Gen Loss: 0.68763876 Disc Loss: 1.402719\n",
      "Gen Loss: 0.6927965 Disc Loss: 1.3907452\n",
      "Gen Loss: 0.69335604 Disc Loss: 1.3902849\n",
      "Gen Loss: 0.693516 Disc Loss: 1.3869846\n",
      "Gen Loss: 0.6947615 Disc Loss: 1.3814614\n",
      "Gen Loss: 0.693894 Disc Loss: 1.3844818\n",
      "Gen Loss: 0.6913845 Disc Loss: 1.3926389\n",
      "Gen Loss: 0.6926907 Disc Loss: 1.3936206\n",
      "Gen Loss: 0.6940571 Disc Loss: 1.3865883\n",
      "Gen Loss: 0.69403666 Disc Loss: 1.3851154\n",
      "Gen Loss: 0.6949088 Disc Loss: 1.384035\n",
      "Gen Loss: 0.6895196 Disc Loss: 1.3984268\n",
      "Gen Loss: 0.6933359 Disc Loss: 1.3880163\n",
      "Gen Loss: 0.6997911 Disc Loss: 1.3753972\n",
      "Gen Loss: 0.6917713 Disc Loss: 1.3934313\n",
      "Gen Loss: 0.6994403 Disc Loss: 1.374606\n",
      "Gen Loss: 0.68822575 Disc Loss: 1.4004546\n",
      "Gen Loss: 0.69405365 Disc Loss: 1.3879695\n",
      "Gen Loss: 0.6988926 Disc Loss: 1.3785993\n",
      "Gen Loss: 0.696347 Disc Loss: 1.3846406\n",
      "Gen Loss: 0.693277 Disc Loss: 1.3868506\n",
      "Gen Loss: 0.6927773 Disc Loss: 1.3885498\n",
      "Gen Loss: 0.6928448 Disc Loss: 1.3883934\n",
      "Gen Loss: 0.69719446 Disc Loss: 1.3796256\n",
      "Gen Loss: 0.694603 Disc Loss: 1.3866701\n",
      "Gen Loss: 0.69040096 Disc Loss: 1.3944256\n",
      "Gen Loss: 0.6940733 Disc Loss: 1.3852966\n",
      "Gen Loss: 0.693993 Disc Loss: 1.3857388\n",
      "Gen Loss: 0.69219625 Disc Loss: 1.3908366\n",
      "Gen Loss: 0.69322455 Disc Loss: 1.3867189\n",
      "Gen Loss: 0.69173485 Disc Loss: 1.3909125\n",
      "Gen Loss: 0.70049113 Disc Loss: 1.3784167\n",
      "Gen Loss: 0.68965805 Disc Loss: 1.3838199\n",
      "Gen Loss: 0.6884675 Disc Loss: 1.402972\n",
      "Gen Loss: 0.695845 Disc Loss: 1.3835788\n",
      "Gen Loss: 0.69944465 Disc Loss: 1.3772844\n",
      "Saved Model\n",
      "Gen Loss: 0.7055266 Disc Loss: 1.3813319\n",
      "Gen Loss: 0.699385 Disc Loss: 1.380531\n",
      "Gen Loss: 0.68825865 Disc Loss: 1.4020808\n",
      "Gen Loss: 0.69400394 Disc Loss: 1.3877282\n",
      "Gen Loss: 0.6949676 Disc Loss: 1.3855646\n",
      "Gen Loss: 0.6923374 Disc Loss: 1.3925519\n",
      "Gen Loss: 0.6959742 Disc Loss: 1.3842113\n",
      "Gen Loss: 0.6952183 Disc Loss: 1.3869976\n",
      "Gen Loss: 0.69434506 Disc Loss: 1.3920326\n",
      "Gen Loss: 0.6975312 Disc Loss: 1.3861028\n",
      "Gen Loss: 0.6962624 Disc Loss: 1.3811386\n",
      "Gen Loss: 0.6946029 Disc Loss: 1.3879137\n",
      "Gen Loss: 0.6934547 Disc Loss: 1.3894196\n",
      "Gen Loss: 0.69451714 Disc Loss: 1.386187\n",
      "Gen Loss: 0.6959603 Disc Loss: 1.3837657\n",
      "Gen Loss: 0.694695 Disc Loss: 1.3856094\n",
      "Gen Loss: 0.69875574 Disc Loss: 1.3800576\n",
      "Gen Loss: 0.6949955 Disc Loss: 1.3861157\n",
      "Gen Loss: 0.69440293 Disc Loss: 1.3904641\n",
      "Gen Loss: 0.69392323 Disc Loss: 1.3898058\n",
      "Gen Loss: 0.696084 Disc Loss: 1.3768845\n",
      "Gen Loss: 0.6985116 Disc Loss: 1.3816233\n",
      "Gen Loss: 0.7193976 Disc Loss: 1.3498172\n",
      "Gen Loss: 0.6959272 Disc Loss: 1.3918257\n",
      "Gen Loss: 0.6952294 Disc Loss: 1.3888791\n",
      "Gen Loss: 0.6951799 Disc Loss: 1.3918381\n",
      "Gen Loss: 0.6911517 Disc Loss: 1.4057548\n",
      "Gen Loss: 0.69978225 Disc Loss: 1.3781747\n",
      "Gen Loss: 0.7067523 Disc Loss: 1.3746161\n",
      "Gen Loss: 0.6855491 Disc Loss: 1.4125104\n",
      "Gen Loss: 0.70804816 Disc Loss: 1.3553808\n",
      "Gen Loss: 0.69609344 Disc Loss: 1.389164\n",
      "Gen Loss: 0.69054246 Disc Loss: 1.3920088\n",
      "Gen Loss: 0.6960385 Disc Loss: 1.3845654\n",
      "Gen Loss: 0.69423765 Disc Loss: 1.3847463\n",
      "Gen Loss: 0.694958 Disc Loss: 1.384971\n",
      "Gen Loss: 0.69345176 Disc Loss: 1.3865414\n",
      "Gen Loss: 0.6893717 Disc Loss: 1.4038873\n",
      "Gen Loss: 0.6926817 Disc Loss: 1.3937061\n",
      "Gen Loss: 0.6936114 Disc Loss: 1.3909718\n",
      "Gen Loss: 0.7014904 Disc Loss: 1.3725547\n",
      "Gen Loss: 0.69348216 Disc Loss: 1.3894892\n",
      "Gen Loss: 0.69997394 Disc Loss: 1.3825408\n",
      "Gen Loss: 0.70324546 Disc Loss: 1.3765457\n",
      "Gen Loss: 0.69119096 Disc Loss: 1.3955808\n",
      "Gen Loss: 0.69542146 Disc Loss: 1.3826818\n",
      "Gen Loss: 0.6924335 Disc Loss: 1.3846078\n",
      "Gen Loss: 0.6987654 Disc Loss: 1.3817557\n",
      "Gen Loss: 0.7011913 Disc Loss: 1.3761127\n",
      "Gen Loss: 0.68764 Disc Loss: 1.3989763\n",
      "Gen Loss: 0.7143756 Disc Loss: 1.3740557\n",
      "Gen Loss: 0.6944967 Disc Loss: 1.3280573\n",
      "Gen Loss: 0.68636954 Disc Loss: 1.4096482\n",
      "Gen Loss: 0.7117467 Disc Loss: 1.3786725\n",
      "Gen Loss: 0.7061749 Disc Loss: 1.374496\n",
      "Gen Loss: 0.7172835 Disc Loss: 1.3677374\n",
      "Gen Loss: 0.6904324 Disc Loss: 1.4010938\n",
      "Gen Loss: 0.69746995 Disc Loss: 1.4024196\n",
      "Gen Loss: 0.70621526 Disc Loss: 1.3710322\n",
      "Gen Loss: 0.701663 Disc Loss: 1.3681244\n",
      "Gen Loss: 0.6914538 Disc Loss: 1.410068\n",
      "Gen Loss: 0.69808364 Disc Loss: 1.3863125\n",
      "Gen Loss: 0.7025931 Disc Loss: 1.3820385\n",
      "Gen Loss: 0.7016915 Disc Loss: 1.3771563\n",
      "Gen Loss: 0.70778453 Disc Loss: 1.3666155\n",
      "Gen Loss: 0.6827332 Disc Loss: 1.3961542\n",
      "Gen Loss: 0.699587 Disc Loss: 1.3668275\n",
      "Gen Loss: 0.6964541 Disc Loss: 1.3904094\n",
      "Gen Loss: 0.7127839 Disc Loss: 1.3678081\n",
      "Gen Loss: 0.7643908 Disc Loss: 1.2728491\n",
      "Gen Loss: 0.69738245 Disc Loss: 1.364977\n",
      "Gen Loss: 0.7023127 Disc Loss: 1.3807516\n",
      "Gen Loss: 0.70605254 Disc Loss: 1.3762207\n",
      "Gen Loss: 0.7269021 Disc Loss: 1.3385725\n",
      "Gen Loss: 0.6709288 Disc Loss: 1.436385\n",
      "Gen Loss: 0.6975808 Disc Loss: 1.3918054\n",
      "Gen Loss: 0.70491743 Disc Loss: 1.4014927\n",
      "Gen Loss: 0.70336616 Disc Loss: 1.3832663\n",
      "Gen Loss: 0.6762736 Disc Loss: 1.4187336\n",
      "Gen Loss: 0.70862186 Disc Loss: 1.3669128\n",
      "Gen Loss: 0.6804699 Disc Loss: 1.4224415\n",
      "Gen Loss: 0.69591033 Disc Loss: 1.3925991\n",
      "Gen Loss: 0.7151922 Disc Loss: 1.3482765\n",
      "Gen Loss: 0.77129674 Disc Loss: 1.2598977\n",
      "Gen Loss: 0.8421725 Disc Loss: 1.1643189\n",
      "Gen Loss: 1.1417689 Disc Loss: 0.85485935\n",
      "Gen Loss: 0.6015605 Disc Loss: 2.0056877\n",
      "Gen Loss: 0.69883263 Disc Loss: 1.500773\n",
      "Gen Loss: 0.8293327 Disc Loss: 1.1711504\n",
      "Gen Loss: 0.6967753 Disc Loss: 1.3158287\n",
      "Gen Loss: 0.72664416 Disc Loss: 1.3390088\n",
      "Gen Loss: 0.7825101 Disc Loss: 1.2559944\n",
      "Gen Loss: 0.6826378 Disc Loss: 1.4792573\n",
      "Gen Loss: 0.7266656 Disc Loss: 1.3552673\n",
      "Gen Loss: 0.7220097 Disc Loss: 1.3805408\n",
      "Gen Loss: 0.94216263 Disc Loss: 1.0503225\n",
      "Gen Loss: 1.0509142 Disc Loss: 0.9775356\n",
      "Gen Loss: 1.4828862 Disc Loss: 0.58778954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.75736386 Disc Loss: 1.3125408\n",
      "Gen Loss: 0.7806827 Disc Loss: 1.3420634\n",
      "Saved Model\n",
      "Gen Loss: 0.7913213 Disc Loss: 1.2449596\n",
      "Gen Loss: 0.6918181 Disc Loss: 1.443482\n",
      "Gen Loss: 0.8003328 Disc Loss: 1.2283832\n",
      "Gen Loss: 0.63508666 Disc Loss: 1.5273585\n",
      "Gen Loss: 0.68154585 Disc Loss: 1.4141388\n",
      "Gen Loss: 0.6363598 Disc Loss: 1.5667435\n",
      "Gen Loss: 0.6969967 Disc Loss: 1.4290779\n",
      "Gen Loss: 0.6671199 Disc Loss: 1.4623396\n",
      "Gen Loss: 0.71322334 Disc Loss: 1.3703623\n",
      "Gen Loss: 0.6767567 Disc Loss: 1.4796152\n",
      "Gen Loss: 0.7332372 Disc Loss: 1.2950633\n",
      "Gen Loss: 0.7416771 Disc Loss: 1.3570256\n",
      "Gen Loss: 0.72162354 Disc Loss: 1.4182429\n",
      "Gen Loss: 0.687133 Disc Loss: 1.4269631\n",
      "Gen Loss: 0.70421296 Disc Loss: 1.4197963\n",
      "Gen Loss: 0.7471038 Disc Loss: 1.328436\n",
      "Gen Loss: 0.7140495 Disc Loss: 1.3359778\n",
      "Gen Loss: 0.7836055 Disc Loss: 1.2607393\n",
      "Gen Loss: 0.7041075 Disc Loss: 1.4914914\n",
      "Gen Loss: 0.6802443 Disc Loss: 1.4588816\n",
      "Gen Loss: 0.6552013 Disc Loss: 1.4396523\n",
      "Gen Loss: 0.6581388 Disc Loss: 1.4742856\n",
      "Gen Loss: 0.6958164 Disc Loss: 1.3941946\n",
      "Gen Loss: 0.6846845 Disc Loss: 1.4137778\n",
      "Gen Loss: 0.6639695 Disc Loss: 1.4573251\n",
      "Gen Loss: 0.69819987 Disc Loss: 1.4002151\n",
      "Gen Loss: 0.70513594 Disc Loss: 1.3662585\n",
      "Gen Loss: 0.6925551 Disc Loss: 1.3890234\n",
      "Gen Loss: 0.6199399 Disc Loss: 1.5141685\n",
      "Gen Loss: 0.7035203 Disc Loss: 1.3950286\n",
      "Gen Loss: 0.7061082 Disc Loss: 1.3951111\n",
      "Gen Loss: 0.71797144 Disc Loss: 1.3442357\n",
      "Gen Loss: 0.72526145 Disc Loss: 1.3595059\n",
      "Gen Loss: 0.6702839 Disc Loss: 1.4571264\n",
      "Gen Loss: 0.716837 Disc Loss: 1.3867354\n",
      "Gen Loss: 0.7109778 Disc Loss: 1.3609751\n",
      "Gen Loss: 0.684816 Disc Loss: 1.4253064\n",
      "Gen Loss: 0.67772204 Disc Loss: 1.4042312\n",
      "Gen Loss: 0.7201078 Disc Loss: 1.353821\n",
      "Gen Loss: 0.7030035 Disc Loss: 1.3984914\n",
      "Gen Loss: 0.69995594 Disc Loss: 1.3849711\n",
      "Gen Loss: 0.6653427 Disc Loss: 1.4771636\n",
      "Gen Loss: 0.6876334 Disc Loss: 1.413869\n",
      "Gen Loss: 0.71289605 Disc Loss: 1.3634224\n",
      "Gen Loss: 0.7471925 Disc Loss: 1.3069977\n",
      "Gen Loss: 0.67308974 Disc Loss: 1.3433177\n",
      "Gen Loss: 0.7485893 Disc Loss: 1.3367617\n",
      "Gen Loss: 0.694054 Disc Loss: 1.4147254\n",
      "Gen Loss: 0.7135665 Disc Loss: 1.367221\n",
      "Gen Loss: 0.68004173 Disc Loss: 1.4283906\n",
      "Gen Loss: 0.7305644 Disc Loss: 1.3579745\n",
      "Gen Loss: 0.70837545 Disc Loss: 1.3615077\n",
      "Gen Loss: 0.6850956 Disc Loss: 1.4515932\n",
      "Gen Loss: 0.6837052 Disc Loss: 1.4628556\n",
      "Gen Loss: 0.75473535 Disc Loss: 1.2911074\n",
      "Gen Loss: 0.6941739 Disc Loss: 1.3915093\n",
      "Gen Loss: 0.72191167 Disc Loss: 1.3401222\n",
      "Gen Loss: 0.6791742 Disc Loss: 1.3998935\n",
      "Gen Loss: 0.71809447 Disc Loss: 1.3545656\n",
      "Gen Loss: 0.7339766 Disc Loss: 1.3396931\n",
      "Gen Loss: 0.7640115 Disc Loss: 1.2809508\n",
      "Gen Loss: 0.74260414 Disc Loss: 1.3796319\n",
      "Gen Loss: 0.6758332 Disc Loss: 1.3262761\n",
      "Gen Loss: 0.74498594 Disc Loss: 1.2850132\n",
      "Gen Loss: 0.7212181 Disc Loss: 1.3386909\n",
      "Gen Loss: 0.723693 Disc Loss: 1.335815\n",
      "Gen Loss: 0.78277135 Disc Loss: 1.2072664\n",
      "Gen Loss: 0.8594538 Disc Loss: 1.1674858\n",
      "Gen Loss: 0.68680704 Disc Loss: 1.4998993\n",
      "Gen Loss: 0.7409538 Disc Loss: 1.3246963\n",
      "Gen Loss: 0.817346 Disc Loss: 1.2205169\n",
      "Gen Loss: 0.8505808 Disc Loss: 1.10309\n",
      "Gen Loss: 0.67195094 Disc Loss: 1.3569915\n",
      "Gen Loss: 0.74397314 Disc Loss: 1.3559203\n",
      "Gen Loss: 0.7690819 Disc Loss: 1.2654074\n",
      "Gen Loss: 0.67558587 Disc Loss: 1.4310614\n",
      "Gen Loss: 0.85056937 Disc Loss: 1.1996282\n",
      "Gen Loss: 0.65157217 Disc Loss: 1.3401991\n",
      "Gen Loss: 0.82888365 Disc Loss: 1.2267287\n",
      "Gen Loss: 0.75756073 Disc Loss: 1.4105471\n",
      "Gen Loss: 0.68289775 Disc Loss: 1.5441251\n",
      "Gen Loss: 0.67080724 Disc Loss: 1.3109452\n",
      "Gen Loss: 0.81173766 Disc Loss: 1.1840115\n",
      "Gen Loss: 0.6944152 Disc Loss: 1.3829525\n",
      "Gen Loss: 0.72424054 Disc Loss: 1.3838832\n",
      "Gen Loss: 0.8505883 Disc Loss: 1.1425911\n",
      "Gen Loss: 0.74034667 Disc Loss: 1.4445554\n",
      "Gen Loss: 0.7306688 Disc Loss: 1.3804991\n",
      "Gen Loss: 0.8165003 Disc Loss: 1.2518454\n",
      "Gen Loss: 0.77278024 Disc Loss: 1.2469311\n",
      "Gen Loss: 0.7677437 Disc Loss: 1.3394768\n",
      "Gen Loss: 0.6706004 Disc Loss: 1.4248672\n",
      "Gen Loss: 0.7329104 Disc Loss: 1.3558143\n",
      "Gen Loss: 0.7740528 Disc Loss: 1.2492545\n",
      "Gen Loss: 0.72930765 Disc Loss: 1.3904455\n",
      "Gen Loss: 0.7084499 Disc Loss: 1.3938847\n",
      "Gen Loss: 0.7519617 Disc Loss: 1.2914228\n",
      "Gen Loss: 0.6760724 Disc Loss: 1.4238786\n",
      "Gen Loss: 0.71206343 Disc Loss: 1.4504099\n",
      "Gen Loss: 0.71558917 Disc Loss: 1.3444734\n",
      "Saved Model\n",
      "Gen Loss: 0.80240333 Disc Loss: 1.2148066\n",
      "Gen Loss: 0.8806227 Disc Loss: 1.1976151\n",
      "Gen Loss: 0.7076975 Disc Loss: 1.3808932\n",
      "Gen Loss: 0.7519437 Disc Loss: 1.3170894\n",
      "Gen Loss: 0.8995312 Disc Loss: 1.1168553\n",
      "Gen Loss: 0.8477864 Disc Loss: 1.4132276\n",
      "Gen Loss: 0.65131456 Disc Loss: 1.4940679\n",
      "Gen Loss: 0.6853027 Disc Loss: 1.462512\n",
      "Gen Loss: 0.671234 Disc Loss: 1.4558986\n",
      "Gen Loss: 0.68685424 Disc Loss: 1.377037\n",
      "Gen Loss: 0.7324691 Disc Loss: 1.3358822\n",
      "Gen Loss: 0.74265337 Disc Loss: 1.3355929\n",
      "Gen Loss: 0.79335964 Disc Loss: 1.2291617\n",
      "Gen Loss: 0.7401736 Disc Loss: 1.4127747\n",
      "Gen Loss: 0.7021415 Disc Loss: 1.3826036\n",
      "Gen Loss: 0.7993711 Disc Loss: 1.2238607\n",
      "Gen Loss: 0.81745756 Disc Loss: 1.2168803\n",
      "Gen Loss: 0.9506093 Disc Loss: 1.0326035\n",
      "Gen Loss: 0.93107367 Disc Loss: 0.91753507\n",
      "Gen Loss: 0.80075157 Disc Loss: 0.8974827\n",
      "Gen Loss: 0.7148791 Disc Loss: 1.409931\n",
      "Gen Loss: 0.81894374 Disc Loss: 1.2154748\n",
      "Gen Loss: 0.7482523 Disc Loss: 1.3295972\n",
      "Gen Loss: 0.71058804 Disc Loss: 1.3236437\n",
      "Gen Loss: 0.6944032 Disc Loss: 1.4094584\n",
      "Gen Loss: 0.7253772 Disc Loss: 1.3608222\n",
      "Gen Loss: 0.6709378 Disc Loss: 1.4726465\n",
      "Gen Loss: 0.7185855 Disc Loss: 1.4069934\n",
      "Gen Loss: 0.7158539 Disc Loss: 1.3886837\n",
      "Gen Loss: 0.7241398 Disc Loss: 1.3992466\n",
      "Gen Loss: 0.6515903 Disc Loss: 1.5047867\n",
      "Gen Loss: 0.6913394 Disc Loss: 1.433453\n",
      "Gen Loss: 0.76192766 Disc Loss: 1.3079414\n",
      "Gen Loss: 0.7019591 Disc Loss: 1.346276\n",
      "Gen Loss: 0.66615057 Disc Loss: 1.4968333\n",
      "Gen Loss: 0.70165944 Disc Loss: 1.3954722\n",
      "Gen Loss: 0.7560375 Disc Loss: 1.3343701\n",
      "Gen Loss: 0.70560515 Disc Loss: 1.4151535\n",
      "Gen Loss: 0.665383 Disc Loss: 1.4204433\n",
      "Gen Loss: 0.65467715 Disc Loss: 1.5116692\n",
      "Gen Loss: 0.6986731 Disc Loss: 1.4093516\n",
      "Gen Loss: 0.64132786 Disc Loss: 1.5263071\n",
      "Gen Loss: 0.6852305 Disc Loss: 1.4192457\n",
      "Gen Loss: 0.735893 Disc Loss: 1.3796382\n",
      "Gen Loss: 0.8161839 Disc Loss: 1.1940511\n",
      "Gen Loss: 0.68928236 Disc Loss: 1.4495562\n",
      "Gen Loss: 0.70587575 Disc Loss: 1.4229541\n",
      "Gen Loss: 0.7705825 Disc Loss: 1.2636011\n",
      "Gen Loss: 0.6032721 Disc Loss: 1.5393131\n",
      "Gen Loss: 0.79598254 Disc Loss: 1.2527002\n",
      "Gen Loss: 0.7318491 Disc Loss: 1.31689\n",
      "Gen Loss: 0.7094736 Disc Loss: 1.3985881\n",
      "Gen Loss: 0.714346 Disc Loss: 1.451978\n",
      "Gen Loss: 0.74598205 Disc Loss: 1.3142272\n",
      "Gen Loss: 0.6366251 Disc Loss: 1.4394162\n",
      "Gen Loss: 0.710986 Disc Loss: 1.4223332\n",
      "Gen Loss: 0.7769237 Disc Loss: 1.2696227\n",
      "Gen Loss: 0.73771405 Disc Loss: 1.3325772\n",
      "Gen Loss: 0.83639973 Disc Loss: 1.2110295\n",
      "Gen Loss: 0.72819597 Disc Loss: 1.2825987\n",
      "Gen Loss: 0.701797 Disc Loss: 1.448457\n",
      "Gen Loss: 0.7013305 Disc Loss: 1.4407092\n",
      "Gen Loss: 0.7589061 Disc Loss: 1.3179243\n",
      "Gen Loss: 0.6972274 Disc Loss: 1.3908415\n",
      "Gen Loss: 0.72809994 Disc Loss: 1.3424572\n",
      "Gen Loss: 0.68535113 Disc Loss: 1.4537772\n",
      "Gen Loss: 0.66606957 Disc Loss: 1.4375784\n",
      "Gen Loss: 0.70085824 Disc Loss: 1.408666\n",
      "Gen Loss: 0.7317131 Disc Loss: 1.3223189\n",
      "Gen Loss: 0.7143011 Disc Loss: 1.3847408\n",
      "Gen Loss: 0.7270657 Disc Loss: 1.3825613\n",
      "Gen Loss: 0.7624879 Disc Loss: 1.2893397\n",
      "Gen Loss: 0.7859192 Disc Loss: 1.2603109\n",
      "Gen Loss: 0.7063755 Disc Loss: 1.4146857\n",
      "Gen Loss: 0.7497002 Disc Loss: 1.3318884\n",
      "Gen Loss: 0.75773364 Disc Loss: 1.3187134\n",
      "Gen Loss: 0.7585701 Disc Loss: 1.3343086\n",
      "Gen Loss: 0.6651775 Disc Loss: 1.3864996\n",
      "Gen Loss: 0.6701294 Disc Loss: 1.4772394\n",
      "Gen Loss: 0.7187153 Disc Loss: 1.3508728\n",
      "Gen Loss: 0.76919746 Disc Loss: 1.2785865\n",
      "Gen Loss: 0.82403916 Disc Loss: 1.1978183\n",
      "Gen Loss: 0.74128884 Disc Loss: 1.3864543\n",
      "Gen Loss: 0.76059777 Disc Loss: 1.3304656\n",
      "Gen Loss: 0.6315216 Disc Loss: 1.5238116\n",
      "Gen Loss: 0.78754425 Disc Loss: 1.3201313\n",
      "Gen Loss: 0.8449837 Disc Loss: 1.1525633\n",
      "Gen Loss: 0.7161925 Disc Loss: 1.5551138\n",
      "Gen Loss: 0.73334086 Disc Loss: 1.3612058\n",
      "Gen Loss: 0.7403193 Disc Loss: 1.4025335\n",
      "Gen Loss: 0.8584324 Disc Loss: 1.1687932\n",
      "Gen Loss: 0.77932954 Disc Loss: 1.3350873\n",
      "Gen Loss: 0.8547082 Disc Loss: 1.0014551\n",
      "Gen Loss: 0.77324855 Disc Loss: 1.3547688\n",
      "Gen Loss: 0.9023509 Disc Loss: 1.1123085\n",
      "Gen Loss: 0.78293693 Disc Loss: 1.2819798\n",
      "Gen Loss: 0.95594823 Disc Loss: 1.0779263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.0573884 Disc Loss: 0.94918317\n",
      "Gen Loss: 0.6364206 Disc Loss: 1.5522293\n",
      "Gen Loss: 0.649357 Disc Loss: 1.4971743\n",
      "Saved Model\n",
      "Gen Loss: 0.724348 Disc Loss: 1.3893864\n",
      "Gen Loss: 0.6919738 Disc Loss: 1.4321823\n",
      "Gen Loss: 0.7511515 Disc Loss: 1.2824154\n",
      "Gen Loss: 0.73674464 Disc Loss: 1.3717452\n",
      "Gen Loss: 0.82516474 Disc Loss: 1.2515912\n",
      "Gen Loss: 0.6928131 Disc Loss: 1.4337959\n",
      "Gen Loss: 0.7848569 Disc Loss: 1.242173\n",
      "Gen Loss: 0.74177146 Disc Loss: 1.3699461\n",
      "Gen Loss: 0.7789142 Disc Loss: 1.2926131\n",
      "Gen Loss: 0.8588 Disc Loss: 1.1582332\n",
      "Gen Loss: 0.8109052 Disc Loss: 1.2852753\n",
      "Gen Loss: 0.89786005 Disc Loss: 1.025532\n",
      "Gen Loss: 0.5976474 Disc Loss: 1.5351663\n",
      "Gen Loss: 0.8045201 Disc Loss: 1.1929926\n",
      "Gen Loss: 0.804762 Disc Loss: 1.298633\n",
      "Gen Loss: 0.75096524 Disc Loss: 1.2469985\n",
      "Gen Loss: 0.730705 Disc Loss: 1.308641\n",
      "Gen Loss: 0.7007909 Disc Loss: 1.4887979\n",
      "Gen Loss: 0.8048172 Disc Loss: 1.2124287\n",
      "Gen Loss: 0.9280665 Disc Loss: 1.2002469\n",
      "Gen Loss: 0.95522773 Disc Loss: 1.1074086\n",
      "Gen Loss: 1.099988 Disc Loss: 0.8218143\n",
      "Gen Loss: 0.86068296 Disc Loss: 0.93903065\n",
      "Gen Loss: 0.8874474 Disc Loss: 1.1843221\n",
      "Gen Loss: 1.1903509 Disc Loss: 0.8126887\n",
      "Gen Loss: 1.0052097 Disc Loss: 1.0270683\n",
      "Gen Loss: 0.7707034 Disc Loss: 1.47263\n",
      "Gen Loss: 0.7924737 Disc Loss: 1.3114147\n",
      "Gen Loss: 0.8279563 Disc Loss: 1.2663937\n",
      "Gen Loss: 0.90300286 Disc Loss: 1.0926499\n",
      "Gen Loss: 0.85476875 Disc Loss: 1.1668959\n",
      "Gen Loss: 0.8528445 Disc Loss: 1.2358487\n",
      "Gen Loss: 0.8689797 Disc Loss: 1.2706462\n",
      "Gen Loss: 0.9769994 Disc Loss: 0.9347719\n",
      "Gen Loss: 1.0149075 Disc Loss: 0.9969776\n",
      "Gen Loss: 0.772221 Disc Loss: 1.345785\n",
      "Gen Loss: 0.9656729 Disc Loss: 1.0365509\n",
      "Gen Loss: 1.0952599 Disc Loss: 0.85448074\n",
      "Gen Loss: 0.99907184 Disc Loss: 0.91437423\n",
      "Gen Loss: 0.9386437 Disc Loss: 1.0802349\n",
      "Gen Loss: 1.0652297 Disc Loss: 0.8804581\n",
      "Gen Loss: 1.0236409 Disc Loss: 1.0636925\n",
      "Gen Loss: 0.77170265 Disc Loss: 1.7754543\n",
      "Gen Loss: 0.95542556 Disc Loss: 1.1804569\n",
      "Gen Loss: 0.9475315 Disc Loss: 1.1682808\n",
      "Gen Loss: 1.2777971 Disc Loss: 0.7562282\n",
      "Gen Loss: 1.213317 Disc Loss: 0.7598436\n",
      "Gen Loss: 0.793586 Disc Loss: 1.3506017\n",
      "Gen Loss: 0.82159114 Disc Loss: 1.1904604\n",
      "Gen Loss: 1.0053402 Disc Loss: 1.0191159\n",
      "Gen Loss: 0.62419987 Disc Loss: 1.5467149\n",
      "Gen Loss: 0.930417 Disc Loss: 1.2581638\n",
      "Gen Loss: 1.0220809 Disc Loss: 0.9894225\n",
      "Gen Loss: 0.89225113 Disc Loss: 1.0429459\n",
      "Gen Loss: 0.9209856 Disc Loss: 1.1139181\n",
      "Gen Loss: 0.9319143 Disc Loss: 1.0890927\n",
      "Gen Loss: 0.55701935 Disc Loss: 1.9880763\n",
      "Gen Loss: 0.780779 Disc Loss: 1.3659956\n",
      "Gen Loss: 0.9415007 Disc Loss: 1.0583048\n",
      "Gen Loss: 0.9255809 Disc Loss: 1.0553939\n",
      "Gen Loss: 1.1742706 Disc Loss: 0.8382598\n",
      "Gen Loss: 0.6530875 Disc Loss: 1.4206206\n",
      "Gen Loss: 1.1207879 Disc Loss: 0.98161614\n",
      "Gen Loss: 0.93949497 Disc Loss: 1.0569685\n",
      "Gen Loss: 1.1201261 Disc Loss: 0.79017806\n",
      "Gen Loss: 0.92336214 Disc Loss: 1.117507\n",
      "Gen Loss: 0.8089819 Disc Loss: 1.2390826\n",
      "Gen Loss: 0.9044123 Disc Loss: 1.2043519\n",
      "Gen Loss: 1.1145821 Disc Loss: 0.94004005\n",
      "Gen Loss: 0.9794644 Disc Loss: 1.1346943\n",
      "Gen Loss: 1.1570563 Disc Loss: 0.8323066\n",
      "Gen Loss: 1.0888104 Disc Loss: 1.0520269\n",
      "Gen Loss: 0.8016326 Disc Loss: 1.3543979\n",
      "Gen Loss: 1.0830112 Disc Loss: 0.9057871\n",
      "Gen Loss: 1.1594615 Disc Loss: 0.7853898\n",
      "Gen Loss: 1.4391894 Disc Loss: 0.6024102\n",
      "Gen Loss: 1.004308 Disc Loss: 0.7731184\n",
      "Gen Loss: 1.0425804 Disc Loss: 1.0457698\n",
      "Gen Loss: 1.3410286 Disc Loss: 0.6817763\n",
      "Gen Loss: 1.0363045 Disc Loss: 0.9889809\n",
      "Gen Loss: 0.6904979 Disc Loss: 1.3674183\n",
      "Gen Loss: 0.68682355 Disc Loss: 1.6599145\n",
      "Gen Loss: 1.1766295 Disc Loss: 0.8655432\n",
      "Gen Loss: 1.5427356 Disc Loss: 0.5284177\n",
      "Gen Loss: 1.8848135 Disc Loss: 0.38938728\n",
      "Gen Loss: 1.4803452 Disc Loss: 0.53578407\n",
      "Gen Loss: 1.3387313 Disc Loss: 0.89458734\n",
      "Gen Loss: 0.8123493 Disc Loss: 1.7380908\n",
      "Gen Loss: 0.6739593 Disc Loss: 1.5362519\n",
      "Gen Loss: 0.78138465 Disc Loss: 1.4112074\n",
      "Gen Loss: 0.9868044 Disc Loss: 1.0847294\n",
      "Gen Loss: 1.1591002 Disc Loss: 0.90352786\n",
      "Gen Loss: 1.0049746 Disc Loss: 1.0289088\n",
      "Gen Loss: 1.0059934 Disc Loss: 1.3067946\n",
      "Gen Loss: 1.158135 Disc Loss: 0.86703587\n",
      "Gen Loss: 1.1014603 Disc Loss: 0.86619145\n",
      "Gen Loss: 1.184793 Disc Loss: 1.2508225\n",
      "Gen Loss: 0.93712497 Disc Loss: 1.8003206\n",
      "Gen Loss: 0.8566075 Disc Loss: 1.0329515\n",
      "Gen Loss: 1.1145462 Disc Loss: 0.7926305\n",
      "Saved Model\n",
      "Gen Loss: 1.1563389 Disc Loss: 0.83816147\n",
      "Gen Loss: 0.73344785 Disc Loss: 1.0495943\n",
      "Gen Loss: 0.9172821 Disc Loss: 1.226563\n",
      "Gen Loss: 0.9337973 Disc Loss: 1.2278461\n",
      "Gen Loss: 0.8800123 Disc Loss: 1.1626264\n",
      "Gen Loss: 0.92437965 Disc Loss: 1.2199955\n",
      "Gen Loss: 1.1815144 Disc Loss: 0.79836845\n",
      "Gen Loss: 1.5847557 Disc Loss: 0.52343863\n",
      "Gen Loss: 1.6432686 Disc Loss: 0.4616004\n",
      "Gen Loss: 1.4973125 Disc Loss: 0.52106583\n",
      "Gen Loss: 1.2976284 Disc Loss: 0.76234114\n",
      "Gen Loss: 0.7414996 Disc Loss: 1.5350772\n",
      "Gen Loss: 1.0339708 Disc Loss: 1.0340489\n",
      "Gen Loss: 1.2761576 Disc Loss: 0.78652716\n",
      "Gen Loss: 1.3789525 Disc Loss: 0.64431655\n",
      "Gen Loss: 1.9550909 Disc Loss: 0.36220786\n",
      "Gen Loss: 1.6973982 Disc Loss: 0.48980677\n",
      "Gen Loss: 1.0351765 Disc Loss: 1.1557292\n",
      "Gen Loss: 0.95938873 Disc Loss: 1.1753528\n",
      "Gen Loss: 1.3665516 Disc Loss: 0.649891\n",
      "Gen Loss: 1.3697963 Disc Loss: 0.75167596\n",
      "Gen Loss: 1.6733661 Disc Loss: 0.53771216\n",
      "Gen Loss: 1.9141979 Disc Loss: 0.39460182\n",
      "Gen Loss: 1.0686857 Disc Loss: 1.1193701\n",
      "Gen Loss: 0.8135301 Disc Loss: 1.342397\n",
      "Gen Loss: 1.0237095 Disc Loss: 0.9492513\n",
      "Gen Loss: 0.9817783 Disc Loss: 1.1635537\n",
      "Gen Loss: 0.7827212 Disc Loss: 1.3392231\n",
      "Gen Loss: 1.1050003 Disc Loss: 1.1220813\n",
      "Gen Loss: 0.9791673 Disc Loss: 1.3136885\n",
      "Gen Loss: 1.0964813 Disc Loss: 1.781889\n",
      "Gen Loss: 0.9383229 Disc Loss: 1.210291\n",
      "Gen Loss: 0.91751367 Disc Loss: 1.3506322\n",
      "Gen Loss: 1.0610722 Disc Loss: 0.9429041\n",
      "Gen Loss: 0.7232897 Disc Loss: 1.6782149\n",
      "Gen Loss: 0.75032014 Disc Loss: 1.376954\n",
      "Gen Loss: 0.91321146 Disc Loss: 1.2188984\n",
      "Gen Loss: 1.1385275 Disc Loss: 0.94375205\n",
      "Gen Loss: 0.7512662 Disc Loss: 1.5862894\n",
      "Gen Loss: 0.991145 Disc Loss: 1.0815413\n",
      "Gen Loss: 1.1560764 Disc Loss: 0.95961064\n",
      "Gen Loss: 1.1864696 Disc Loss: 0.8579291\n",
      "Gen Loss: 1.5095437 Disc Loss: 0.7853308\n",
      "Gen Loss: 0.99122417 Disc Loss: 1.4326265\n",
      "Gen Loss: 1.1884842 Disc Loss: 1.0045184\n",
      "Gen Loss: 1.3186835 Disc Loss: 0.7463901\n",
      "Gen Loss: 1.297223 Disc Loss: 0.86802006\n",
      "Gen Loss: 1.3564091 Disc Loss: 0.8705734\n",
      "Gen Loss: 0.5483614 Disc Loss: 3.009616\n",
      "Gen Loss: 1.221216 Disc Loss: 1.110127\n",
      "Gen Loss: 0.90866125 Disc Loss: 1.2497256\n",
      "Gen Loss: 0.94580865 Disc Loss: 1.1824223\n",
      "Gen Loss: 0.69845164 Disc Loss: 1.7483511\n",
      "Gen Loss: 0.75076073 Disc Loss: 1.428278\n",
      "Gen Loss: 0.94080853 Disc Loss: 1.1221224\n",
      "Gen Loss: 0.68081605 Disc Loss: 1.6376587\n",
      "Gen Loss: 0.66963726 Disc Loss: 1.6335717\n",
      "Gen Loss: 0.8063042 Disc Loss: 1.2595901\n",
      "Gen Loss: 0.86277974 Disc Loss: 1.204963\n",
      "Gen Loss: 1.0257676 Disc Loss: 0.9373274\n",
      "Gen Loss: 1.1250851 Disc Loss: 0.99211097\n",
      "Gen Loss: 1.1086941 Disc Loss: 0.97904944\n",
      "Gen Loss: 1.3056585 Disc Loss: 0.78119445\n",
      "Gen Loss: 0.9943657 Disc Loss: 1.1689725\n",
      "Gen Loss: 0.9781283 Disc Loss: 1.065875\n",
      "Gen Loss: 0.7319743 Disc Loss: 1.5453776\n",
      "Gen Loss: 1.0330482 Disc Loss: 0.97083485\n",
      "Gen Loss: 0.803591 Disc Loss: 1.3035979\n",
      "Gen Loss: 0.5960542 Disc Loss: 1.4253418\n",
      "Gen Loss: 1.0976942 Disc Loss: 0.96418923\n",
      "Gen Loss: 1.3952456 Disc Loss: 0.62740576\n",
      "Gen Loss: 1.2627411 Disc Loss: 0.7283087\n",
      "Gen Loss: 1.127358 Disc Loss: 0.90676236\n",
      "Gen Loss: 0.8412461 Disc Loss: 1.3055539\n",
      "Gen Loss: 0.9826664 Disc Loss: 0.89353395\n",
      "Gen Loss: 0.97765875 Disc Loss: 1.0470575\n",
      "Gen Loss: 1.186255 Disc Loss: 0.75094676\n",
      "Gen Loss: 1.4774902 Disc Loss: 0.61259437\n",
      "Gen Loss: 0.60542953 Disc Loss: 1.7228646\n",
      "Gen Loss: 0.9207254 Disc Loss: 1.1530355\n",
      "Gen Loss: 0.9390306 Disc Loss: 1.0644429\n",
      "Gen Loss: 0.9775635 Disc Loss: 1.079952\n",
      "Gen Loss: 0.99371016 Disc Loss: 1.1121666\n",
      "Gen Loss: 0.7555558 Disc Loss: 1.4566731\n",
      "Gen Loss: 1.0319369 Disc Loss: 1.0560985\n",
      "Gen Loss: 1.160864 Disc Loss: 0.97882783\n",
      "Gen Loss: 0.59928447 Disc Loss: 1.9164574\n",
      "Gen Loss: 1.2447197 Disc Loss: 0.78090674\n",
      "Gen Loss: 1.2159439 Disc Loss: 0.829789\n",
      "Gen Loss: 0.8324387 Disc Loss: 1.356888\n",
      "Gen Loss: 0.7618 Disc Loss: 1.4910284\n",
      "Gen Loss: 0.64937085 Disc Loss: 1.5311736\n",
      "Gen Loss: 0.7799789 Disc Loss: 1.3188992\n",
      "Gen Loss: 0.8702849 Disc Loss: 1.5646615\n",
      "Gen Loss: 0.89144695 Disc Loss: 1.1704532\n",
      "Gen Loss: 0.96469384 Disc Loss: 1.1260748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.0024402 Disc Loss: 1.0906537\n",
      "Gen Loss: 0.90100145 Disc Loss: 1.2592301\n",
      "Gen Loss: 1.1443319 Disc Loss: 0.98285156\n",
      "Gen Loss: 0.8215074 Disc Loss: 1.4432495\n",
      "Saved Model\n",
      "Gen Loss: 0.7992187 Disc Loss: 1.062444\n",
      "Gen Loss: 1.2186761 Disc Loss: 0.9304035\n",
      "Gen Loss: 0.8787674 Disc Loss: 1.3406912\n",
      "Gen Loss: 1.5559711 Disc Loss: 0.5129785\n",
      "Gen Loss: 1.3165828 Disc Loss: 0.6770245\n",
      "Gen Loss: 1.5032082 Disc Loss: 0.59459865\n",
      "Gen Loss: 1.2545028 Disc Loss: 0.9786785\n",
      "Gen Loss: 0.40632325 Disc Loss: 3.1154923\n",
      "Gen Loss: 1.2814169 Disc Loss: 0.8566958\n",
      "Gen Loss: 0.7037898 Disc Loss: 1.1955562\n",
      "Gen Loss: 0.7182567 Disc Loss: 1.4480157\n",
      "Gen Loss: 1.0331936 Disc Loss: 1.003111\n",
      "Gen Loss: 0.92486405 Disc Loss: 1.228148\n",
      "Gen Loss: 0.93282545 Disc Loss: 1.2028477\n",
      "Gen Loss: 0.7893822 Disc Loss: 1.4915345\n",
      "Gen Loss: 0.9987251 Disc Loss: 1.3491124\n",
      "Gen Loss: 1.0612108 Disc Loss: 0.9798697\n",
      "Gen Loss: 1.064238 Disc Loss: 1.0228109\n",
      "Gen Loss: 0.7358339 Disc Loss: 1.583911\n",
      "Gen Loss: 1.1134278 Disc Loss: 1.0221238\n",
      "Gen Loss: 0.78774285 Disc Loss: 1.4441378\n",
      "Gen Loss: 1.6712621 Disc Loss: 0.48637918\n",
      "Gen Loss: 1.2939689 Disc Loss: 0.821524\n",
      "Gen Loss: 0.6476357 Disc Loss: 1.9025346\n",
      "Gen Loss: 0.6908405 Disc Loss: 1.3913074\n",
      "Gen Loss: 0.60895026 Disc Loss: 1.9242654\n",
      "Gen Loss: 0.8749514 Disc Loss: 1.3753858\n",
      "Gen Loss: 0.721377 Disc Loss: 1.5232452\n",
      "Gen Loss: 0.5963974 Disc Loss: 1.688759\n",
      "Gen Loss: 0.66407 Disc Loss: 1.5140915\n",
      "Gen Loss: 0.8380724 Disc Loss: 1.2494395\n",
      "Gen Loss: 0.9579146 Disc Loss: 1.1022761\n",
      "Gen Loss: 0.82028145 Disc Loss: 1.4056739\n",
      "Gen Loss: 0.8692144 Disc Loss: 1.1227708\n",
      "Gen Loss: 0.6732739 Disc Loss: 1.9779886\n",
      "Gen Loss: 1.1770458 Disc Loss: 1.1463695\n",
      "Gen Loss: 1.1479845 Disc Loss: 0.92666817\n",
      "Gen Loss: 1.1595861 Disc Loss: 0.8640041\n",
      "Gen Loss: 0.86232865 Disc Loss: 1.3413253\n",
      "Gen Loss: 0.8032719 Disc Loss: 1.5386636\n",
      "Gen Loss: 0.56907547 Disc Loss: 1.8648964\n",
      "Gen Loss: 0.6902166 Disc Loss: 1.5371741\n",
      "Gen Loss: 0.7448455 Disc Loss: 1.3702837\n",
      "Gen Loss: 0.78958607 Disc Loss: 1.3107766\n",
      "Gen Loss: 0.6424255 Disc Loss: 1.5295105\n",
      "Gen Loss: 0.7631397 Disc Loss: 1.4415064\n",
      "Gen Loss: 0.76134616 Disc Loss: 1.6137302\n",
      "Gen Loss: 0.7967703 Disc Loss: 1.4717615\n",
      "Gen Loss: 0.97320235 Disc Loss: 1.3363628\n",
      "Gen Loss: 0.9029168 Disc Loss: 1.0513275\n",
      "Gen Loss: 0.8042579 Disc Loss: 1.2233669\n",
      "Gen Loss: 0.8275834 Disc Loss: 1.2651694\n",
      "Gen Loss: 0.72053266 Disc Loss: 1.4525008\n",
      "Gen Loss: 0.70340204 Disc Loss: 1.7049501\n",
      "Gen Loss: 0.9770907 Disc Loss: 1.1430622\n",
      "Gen Loss: 0.9662827 Disc Loss: 1.1100094\n",
      "Gen Loss: 1.0952997 Disc Loss: 0.8754554\n",
      "Gen Loss: 0.8297315 Disc Loss: 1.2488513\n",
      "Gen Loss: 0.9176768 Disc Loss: 1.1784801\n",
      "Gen Loss: 0.49289495 Disc Loss: 2.1579552\n",
      "Gen Loss: 0.66991544 Disc Loss: 1.8277531\n",
      "Gen Loss: 0.9569708 Disc Loss: 1.1019949\n",
      "Gen Loss: 0.7408674 Disc Loss: 1.5207949\n",
      "Gen Loss: 0.76034987 Disc Loss: 1.4353216\n",
      "Gen Loss: 0.5796177 Disc Loss: 1.6754208\n",
      "Gen Loss: 0.6550672 Disc Loss: 1.6794405\n",
      "Gen Loss: 0.64720607 Disc Loss: 1.5551003\n",
      "Gen Loss: 0.6807779 Disc Loss: 1.5626178\n",
      "Gen Loss: 0.7487577 Disc Loss: 1.2996752\n",
      "Gen Loss: 0.8868252 Disc Loss: 1.1557114\n",
      "Gen Loss: 0.87187386 Disc Loss: 1.1585417\n",
      "Gen Loss: 0.8103223 Disc Loss: 1.3244344\n",
      "Gen Loss: 0.62167925 Disc Loss: 1.6497723\n",
      "Gen Loss: 0.5710018 Disc Loss: 1.642679\n",
      "Gen Loss: 0.8030913 Disc Loss: 1.2470272\n",
      "Gen Loss: 0.9833441 Disc Loss: 0.9973693\n",
      "Gen Loss: 0.7439813 Disc Loss: 1.340743\n",
      "Gen Loss: 0.6168178 Disc Loss: 1.6764417\n",
      "Gen Loss: 0.6451767 Disc Loss: 1.73929\n",
      "Gen Loss: 0.75559306 Disc Loss: 1.1615542\n",
      "Gen Loss: 0.7981663 Disc Loss: 1.3105273\n",
      "Gen Loss: 0.683578 Disc Loss: 1.3897355\n",
      "Gen Loss: 0.82400537 Disc Loss: 1.2129985\n",
      "Gen Loss: 0.87232625 Disc Loss: 1.1198541\n",
      "Gen Loss: 0.70336044 Disc Loss: 1.3361017\n",
      "Gen Loss: 0.71884674 Disc Loss: 1.4805658\n",
      "Gen Loss: 0.8035282 Disc Loss: 1.2949653\n",
      "Gen Loss: 0.71125114 Disc Loss: 1.364454\n",
      "Gen Loss: 0.5937373 Disc Loss: 1.7647083\n",
      "Gen Loss: 0.79868543 Disc Loss: 1.3270798\n",
      "Gen Loss: 0.9701768 Disc Loss: 1.0388103\n",
      "Gen Loss: 0.84947574 Disc Loss: 1.1904643\n",
      "Gen Loss: 0.96113443 Disc Loss: 1.3971077\n",
      "Gen Loss: 0.9023933 Disc Loss: 1.2113085\n",
      "Gen Loss: 0.7180947 Disc Loss: 1.453634\n",
      "Gen Loss: 0.6913966 Disc Loss: 1.4379834\n",
      "Gen Loss: 0.74537694 Disc Loss: 1.4136137\n",
      "Gen Loss: 0.72222805 Disc Loss: 1.4341167\n",
      "Gen Loss: 0.8407409 Disc Loss: 1.1981416\n",
      "Gen Loss: 0.72527945 Disc Loss: 1.3632078\n",
      "Saved Model\n",
      "Gen Loss: 0.6719744 Disc Loss: 1.5951414\n",
      "Gen Loss: 0.98197067 Disc Loss: 1.1646817\n",
      "Gen Loss: 0.9265506 Disc Loss: 1.1363921\n",
      "Gen Loss: 1.427114 Disc Loss: 0.64674306\n",
      "Gen Loss: 1.5886409 Disc Loss: 0.4906311\n",
      "Gen Loss: 1.688308 Disc Loss: 0.46375299\n",
      "Gen Loss: 0.9014946 Disc Loss: 1.3693674\n",
      "Gen Loss: 0.9698427 Disc Loss: 1.2254599\n",
      "Gen Loss: 0.6711714 Disc Loss: 1.7973697\n",
      "Gen Loss: 0.75899506 Disc Loss: 1.498153\n",
      "Gen Loss: 0.9356054 Disc Loss: 1.0978832\n",
      "Gen Loss: 0.8131055 Disc Loss: 1.2564026\n",
      "Gen Loss: 0.7606917 Disc Loss: 1.6731956\n",
      "Gen Loss: 1.0025084 Disc Loss: 1.055304\n",
      "Gen Loss: 1.349262 Disc Loss: 0.8550539\n",
      "Gen Loss: 0.9748486 Disc Loss: 1.103757\n",
      "Gen Loss: 1.6249614 Disc Loss: 0.5041777\n",
      "Gen Loss: 1.2782822 Disc Loss: 0.5938781\n",
      "Gen Loss: 0.8296313 Disc Loss: 1.4712646\n",
      "Gen Loss: 0.6001891 Disc Loss: 1.8438823\n",
      "Gen Loss: 0.5348742 Disc Loss: 1.8951694\n",
      "Gen Loss: 0.63561714 Disc Loss: 1.644314\n",
      "Gen Loss: 0.74779344 Disc Loss: 1.3914819\n",
      "Gen Loss: 0.8441719 Disc Loss: 1.225486\n",
      "Gen Loss: 0.88428533 Disc Loss: 1.1673648\n",
      "Gen Loss: 0.7464811 Disc Loss: 1.3799156\n",
      "Gen Loss: 0.84202945 Disc Loss: 1.2886597\n",
      "Gen Loss: 0.7725464 Disc Loss: 1.3432777\n",
      "Gen Loss: 0.46072656 Disc Loss: 2.186799\n",
      "Gen Loss: 0.62110823 Disc Loss: 1.6817712\n",
      "Gen Loss: 0.64177215 Disc Loss: 1.5492702\n",
      "Gen Loss: 0.7715062 Disc Loss: 1.407922\n",
      "Gen Loss: 0.74426264 Disc Loss: 1.4116173\n",
      "Gen Loss: 0.8113072 Disc Loss: 1.2140296\n",
      "Gen Loss: 0.8404811 Disc Loss: 1.2743773\n",
      "Gen Loss: 0.8370921 Disc Loss: 1.2620764\n",
      "Gen Loss: 0.7913661 Disc Loss: 1.3087529\n",
      "Gen Loss: 0.7585205 Disc Loss: 1.289432\n",
      "Gen Loss: 0.95271873 Disc Loss: 1.1486425\n",
      "Gen Loss: 0.8520653 Disc Loss: 1.2034304\n",
      "Gen Loss: 0.93497515 Disc Loss: 1.260438\n",
      "Gen Loss: 0.8288958 Disc Loss: 1.3373306\n",
      "Gen Loss: 1.0972576 Disc Loss: 1.0403323\n",
      "Gen Loss: 0.9159373 Disc Loss: 1.1256733\n",
      "Gen Loss: 0.5245247 Disc Loss: 1.943773\n",
      "Gen Loss: 0.6451291 Disc Loss: 1.6009034\n",
      "Gen Loss: 0.55027926 Disc Loss: 1.5083582\n",
      "Gen Loss: 0.7887989 Disc Loss: 1.4099317\n",
      "Gen Loss: 0.843598 Disc Loss: 1.2093372\n",
      "Gen Loss: 1.0116749 Disc Loss: 1.0407693\n",
      "Gen Loss: 0.8035935 Disc Loss: 1.2493169\n",
      "Gen Loss: 1.0285664 Disc Loss: 1.0548701\n",
      "Gen Loss: 0.8773912 Disc Loss: 1.3926597\n",
      "Gen Loss: 0.81962645 Disc Loss: 1.3652302\n",
      "Gen Loss: 0.47080588 Disc Loss: 2.0647616\n",
      "Gen Loss: 0.7139603 Disc Loss: 1.5908227\n",
      "Gen Loss: 0.8998268 Disc Loss: 1.1987479\n",
      "Gen Loss: 1.1237756 Disc Loss: 0.8367087\n",
      "Gen Loss: 1.0979445 Disc Loss: 0.8988291\n",
      "Gen Loss: 1.0046902 Disc Loss: 0.95315576\n",
      "Gen Loss: 0.9543445 Disc Loss: 1.0726943\n",
      "Gen Loss: 0.5967082 Disc Loss: 1.8160136\n",
      "Gen Loss: 0.7340496 Disc Loss: 1.5347707\n",
      "Gen Loss: 0.5903052 Disc Loss: 1.7870746\n",
      "Gen Loss: 0.76250386 Disc Loss: 1.5286618\n",
      "Gen Loss: 0.7947537 Disc Loss: 1.4134083\n",
      "Gen Loss: 0.90506184 Disc Loss: 1.1973443\n",
      "Gen Loss: 0.9184338 Disc Loss: 1.3689706\n",
      "Gen Loss: 0.78536284 Disc Loss: 1.2579484\n",
      "Gen Loss: 1.2291162 Disc Loss: 0.87406534\n",
      "Gen Loss: 0.7183618 Disc Loss: 1.600421\n",
      "Gen Loss: 0.56350553 Disc Loss: 1.886206\n",
      "Gen Loss: 0.78046286 Disc Loss: 1.4076475\n",
      "Gen Loss: 0.66914815 Disc Loss: 1.6096133\n",
      "Gen Loss: 0.87333333 Disc Loss: 1.2044104\n",
      "Gen Loss: 0.8677231 Disc Loss: 1.2709168\n",
      "Gen Loss: 0.7606163 Disc Loss: 1.3117557\n",
      "Gen Loss: 0.7904601 Disc Loss: 1.3167107\n",
      "Gen Loss: 0.8678647 Disc Loss: 1.193285\n",
      "Gen Loss: 0.83909786 Disc Loss: 1.288971\n",
      "Gen Loss: 0.61539805 Disc Loss: 1.6213586\n",
      "Gen Loss: 0.8458617 Disc Loss: 1.1715777\n",
      "Gen Loss: 0.85390365 Disc Loss: 1.3705175\n",
      "Gen Loss: 0.8079299 Disc Loss: 1.2257261\n",
      "Gen Loss: 0.87752366 Disc Loss: 1.203997\n",
      "Gen Loss: 0.82303035 Disc Loss: 1.2367344\n",
      "Gen Loss: 0.7118029 Disc Loss: 1.4768769\n",
      "Gen Loss: 0.6782776 Disc Loss: 1.486953\n",
      "Gen Loss: 0.74312186 Disc Loss: 1.4102827\n",
      "Gen Loss: 0.70807177 Disc Loss: 1.5523672\n",
      "Gen Loss: 0.9862761 Disc Loss: 1.1193144\n",
      "Gen Loss: 0.9478926 Disc Loss: 1.0839946\n",
      "Gen Loss: 1.0879018 Disc Loss: 0.9107091\n",
      "Gen Loss: 0.6602003 Disc Loss: 1.6881568\n",
      "Gen Loss: 0.8228508 Disc Loss: 1.3277503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.759938 Disc Loss: 1.3365281\n",
      "Gen Loss: 0.6827011 Disc Loss: 1.5371606\n",
      "Gen Loss: 0.92681044 Disc Loss: 1.1035191\n",
      "Gen Loss: 0.87831277 Disc Loss: 1.1560967\n",
      "Gen Loss: 0.7611102 Disc Loss: 1.2849616\n",
      "Saved Model\n",
      "Gen Loss: 0.78852415 Disc Loss: 1.3856788\n",
      "Gen Loss: 0.92076635 Disc Loss: 1.1398218\n",
      "Gen Loss: 1.0523453 Disc Loss: 0.9095418\n",
      "Gen Loss: 1.0604268 Disc Loss: 0.954324\n",
      "Gen Loss: 0.56676483 Disc Loss: 2.0069737\n",
      "Gen Loss: 1.1452739 Disc Loss: 0.94373524\n",
      "Gen Loss: 0.77479273 Disc Loss: 1.3679459\n",
      "Gen Loss: 0.62884736 Disc Loss: 1.7802746\n",
      "Gen Loss: 1.0816095 Disc Loss: 0.9854312\n",
      "Gen Loss: 1.2089243 Disc Loss: 0.80785537\n",
      "Gen Loss: 1.488889 Disc Loss: 0.5925215\n",
      "Gen Loss: 1.656277 Disc Loss: 0.45818847\n",
      "Gen Loss: 0.749002 Disc Loss: 1.4458678\n",
      "Gen Loss: 0.71021855 Disc Loss: 1.4854252\n",
      "Gen Loss: 0.62730545 Disc Loss: 1.6810776\n",
      "Gen Loss: 0.56728494 Disc Loss: 1.8458505\n",
      "Gen Loss: 0.6361406 Disc Loss: 1.6255366\n",
      "Gen Loss: 0.7879378 Disc Loss: 1.3450919\n",
      "Gen Loss: 0.8867948 Disc Loss: 1.1696687\n",
      "Gen Loss: 0.92090446 Disc Loss: 1.0803466\n",
      "Gen Loss: 0.90870583 Disc Loss: 1.1770182\n",
      "Gen Loss: 0.6053461 Disc Loss: 1.6939085\n",
      "Gen Loss: 0.8203331 Disc Loss: 1.427671\n",
      "Gen Loss: 0.8269911 Disc Loss: 1.3054507\n",
      "Gen Loss: 0.998456 Disc Loss: 1.0573432\n",
      "Gen Loss: 0.79519916 Disc Loss: 1.3894248\n",
      "Gen Loss: 0.985543 Disc Loss: 1.1327231\n",
      "Gen Loss: 1.0093656 Disc Loss: 0.96845853\n",
      "Gen Loss: 0.71483254 Disc Loss: 1.888093\n",
      "Gen Loss: 0.75733966 Disc Loss: 1.4577265\n",
      "Gen Loss: 0.9478922 Disc Loss: 1.2662733\n",
      "Gen Loss: 1.0066342 Disc Loss: 1.0485117\n",
      "Gen Loss: 0.9623339 Disc Loss: 1.2132579\n",
      "Gen Loss: 0.70866954 Disc Loss: 1.77473\n",
      "Gen Loss: 0.8319333 Disc Loss: 1.2093327\n",
      "Gen Loss: 0.883577 Disc Loss: 1.2116013\n",
      "Gen Loss: 0.747826 Disc Loss: 1.393699\n",
      "Gen Loss: 0.5802182 Disc Loss: 1.7738198\n",
      "Gen Loss: 0.70521927 Disc Loss: 1.4946581\n",
      "Gen Loss: 0.76982903 Disc Loss: 1.3273865\n",
      "Gen Loss: 0.79558265 Disc Loss: 1.2404637\n",
      "Gen Loss: 0.74215484 Disc Loss: 1.3439637\n",
      "Gen Loss: 0.7203214 Disc Loss: 1.4202694\n",
      "Gen Loss: 0.90978205 Disc Loss: 1.2100158\n",
      "Gen Loss: 1.1246514 Disc Loss: 0.8036551\n",
      "Gen Loss: 0.58683276 Disc Loss: 1.7469555\n",
      "Gen Loss: 0.7790706 Disc Loss: 1.3779602\n",
      "Gen Loss: 0.9791364 Disc Loss: 1.0392039\n",
      "Gen Loss: 0.9615369 Disc Loss: 0.948815\n",
      "Gen Loss: 1.0099411 Disc Loss: 1.0127568\n",
      "Gen Loss: 0.7816851 Disc Loss: 1.3675454\n",
      "Gen Loss: 0.7141102 Disc Loss: 1.5041962\n",
      "Gen Loss: 0.75869703 Disc Loss: 1.4276508\n",
      "Gen Loss: 0.8464037 Disc Loss: 1.1956581\n",
      "Gen Loss: 0.9935776 Disc Loss: 1.015782\n",
      "Gen Loss: 0.9953096 Disc Loss: 1.0202752\n",
      "Gen Loss: 0.84067714 Disc Loss: 1.2943943\n",
      "Gen Loss: 0.9198108 Disc Loss: 1.1690861\n",
      "Gen Loss: 0.788698 Disc Loss: 1.5216516\n",
      "Gen Loss: 1.1891499 Disc Loss: 1.0566329\n",
      "Gen Loss: 0.7128739 Disc Loss: 1.4574993\n",
      "Gen Loss: 0.6893469 Disc Loss: 1.492115\n",
      "Gen Loss: 0.6256373 Disc Loss: 1.9771643\n",
      "Gen Loss: 0.80380785 Disc Loss: 1.345561\n",
      "Gen Loss: 0.6451881 Disc Loss: 1.5036395\n",
      "Gen Loss: 0.55409724 Disc Loss: 1.8707231\n",
      "Gen Loss: 0.7380706 Disc Loss: 1.436562\n",
      "Gen Loss: 0.7149031 Disc Loss: 1.5065322\n",
      "Gen Loss: 0.8215722 Disc Loss: 1.2533393\n",
      "Gen Loss: 0.7398489 Disc Loss: 1.2988461\n",
      "Gen Loss: 0.7123342 Disc Loss: 1.4843152\n",
      "Gen Loss: 0.75146806 Disc Loss: 1.3852735\n",
      "Gen Loss: 0.65672123 Disc Loss: 1.601588\n",
      "Gen Loss: 0.64510846 Disc Loss: 1.6619527\n",
      "Gen Loss: 1.1071253 Disc Loss: 1.022614\n",
      "Gen Loss: 0.8649047 Disc Loss: 1.1955395\n",
      "Gen Loss: 1.0999503 Disc Loss: 0.8595854\n",
      "Gen Loss: 0.9062098 Disc Loss: 1.1315486\n",
      "Gen Loss: 0.67674494 Disc Loss: 1.6491141\n",
      "Gen Loss: 0.6045614 Disc Loss: 1.785553\n",
      "Gen Loss: 0.59884083 Disc Loss: 1.6704192\n",
      "Gen Loss: 0.64480895 Disc Loss: 1.5585129\n",
      "Gen Loss: 0.75092566 Disc Loss: 1.352154\n",
      "Gen Loss: 0.680077 Disc Loss: 1.5362155\n",
      "Gen Loss: 0.7814656 Disc Loss: 1.3998401\n",
      "Gen Loss: 0.79163957 Disc Loss: 1.2377295\n",
      "Gen Loss: 0.7000123 Disc Loss: 1.4620919\n",
      "Gen Loss: 0.8370143 Disc Loss: 1.2011583\n",
      "Gen Loss: 0.8359129 Disc Loss: 1.1425915\n",
      "Gen Loss: 0.76616716 Disc Loss: 1.4447948\n",
      "Gen Loss: 0.7548112 Disc Loss: 1.3396641\n",
      "Gen Loss: 0.83992136 Disc Loss: 1.3177185\n",
      "Gen Loss: 0.69381857 Disc Loss: 1.4202847\n",
      "Gen Loss: 0.6807016 Disc Loss: 1.4894608\n",
      "Gen Loss: 0.7058854 Disc Loss: 1.431774\n",
      "Gen Loss: 0.7552624 Disc Loss: 1.3986762\n",
      "Gen Loss: 0.82796043 Disc Loss: 1.3084462\n",
      "Gen Loss: 0.7695892 Disc Loss: 1.2662914\n",
      "Gen Loss: 0.945856 Disc Loss: 1.1021022\n",
      "Gen Loss: 1.1088172 Disc Loss: 0.97609466\n",
      "Saved Model\n",
      "Gen Loss: 1.0032964 Disc Loss: 1.1570356\n",
      "Gen Loss: 1.079619 Disc Loss: 1.1051419\n",
      "Gen Loss: 0.8566402 Disc Loss: 1.2000722\n",
      "Gen Loss: 1.1123602 Disc Loss: 0.9491215\n",
      "Gen Loss: 1.2079605 Disc Loss: 0.93517065\n",
      "Gen Loss: 1.0002834 Disc Loss: 0.9841007\n",
      "Gen Loss: 1.3830326 Disc Loss: 0.7884706\n",
      "Gen Loss: 0.7779583 Disc Loss: 1.8614697\n",
      "Gen Loss: 0.7421203 Disc Loss: 1.522856\n",
      "Gen Loss: 1.0640466 Disc Loss: 0.95031226\n",
      "Gen Loss: 0.95495576 Disc Loss: 1.1565351\n",
      "Gen Loss: 0.63905257 Disc Loss: 1.6505705\n",
      "Gen Loss: 0.75353825 Disc Loss: 1.3749492\n",
      "Gen Loss: 0.71449006 Disc Loss: 1.4094981\n",
      "Gen Loss: 0.85810083 Disc Loss: 1.2287924\n",
      "Gen Loss: 1.0655925 Disc Loss: 0.9524843\n",
      "Gen Loss: 1.2232647 Disc Loss: 0.82035077\n",
      "Gen Loss: 0.98697406 Disc Loss: 1.0402783\n",
      "Gen Loss: 1.2107973 Disc Loss: 0.8988915\n",
      "Gen Loss: 0.7765206 Disc Loss: 1.3135493\n",
      "Gen Loss: 0.68064624 Disc Loss: 1.5982461\n",
      "Gen Loss: 0.87327147 Disc Loss: 1.2737787\n",
      "Gen Loss: 0.9861964 Disc Loss: 1.0551409\n",
      "Gen Loss: 1.0923355 Disc Loss: 0.9117565\n",
      "Gen Loss: 1.027733 Disc Loss: 1.0626967\n",
      "Gen Loss: 0.86856997 Disc Loss: 1.0996798\n",
      "Gen Loss: 0.48862597 Disc Loss: 2.0619547\n",
      "Gen Loss: 0.57110804 Disc Loss: 1.7756268\n",
      "Gen Loss: 0.57486296 Disc Loss: 1.897303\n",
      "Gen Loss: 0.72452897 Disc Loss: 1.4316537\n",
      "Gen Loss: 0.9189807 Disc Loss: 1.1549425\n",
      "Gen Loss: 0.86229396 Disc Loss: 1.3065078\n",
      "Gen Loss: 0.7638416 Disc Loss: 1.3691764\n",
      "Gen Loss: 0.86519074 Disc Loss: 1.3483526\n",
      "Gen Loss: 1.1101156 Disc Loss: 0.8607671\n",
      "Gen Loss: 1.0759178 Disc Loss: 0.88312197\n",
      "Gen Loss: 1.1993802 Disc Loss: 0.78969455\n",
      "Gen Loss: 1.0072274 Disc Loss: 1.0476444\n",
      "Gen Loss: 0.9881891 Disc Loss: 1.1051041\n",
      "Gen Loss: 1.1746893 Disc Loss: 0.8810182\n",
      "Gen Loss: 0.9375487 Disc Loss: 1.2815586\n",
      "Gen Loss: 1.204782 Disc Loss: 0.9149739\n",
      "Gen Loss: 0.75358295 Disc Loss: 1.5390708\n",
      "Gen Loss: 0.61883944 Disc Loss: 1.6905508\n",
      "Gen Loss: 1.0677669 Disc Loss: 0.94169265\n",
      "Gen Loss: 1.3361943 Disc Loss: 0.65110266\n",
      "Gen Loss: 0.9263536 Disc Loss: 1.3703305\n",
      "Gen Loss: 1.3216435 Disc Loss: 0.73170054\n",
      "Gen Loss: 1.5037856 Disc Loss: 0.57235944\n",
      "Gen Loss: 1.469499 Disc Loss: 0.61634773\n",
      "Gen Loss: 0.96308 Disc Loss: 1.1055561\n",
      "Gen Loss: 0.7909564 Disc Loss: 1.383762\n",
      "Gen Loss: 0.78771293 Disc Loss: 1.2873216\n",
      "Gen Loss: 0.9072379 Disc Loss: 1.2043945\n",
      "Gen Loss: 0.68294436 Disc Loss: 1.4837348\n",
      "Gen Loss: 0.89295506 Disc Loss: 1.1558216\n",
      "Gen Loss: 0.818291 Disc Loss: 1.3382925\n",
      "Gen Loss: 0.85181665 Disc Loss: 1.1754668\n",
      "Gen Loss: 0.60165656 Disc Loss: 1.9905528\n",
      "Gen Loss: 0.7420648 Disc Loss: 1.402939\n",
      "Gen Loss: 0.823198 Disc Loss: 1.249145\n",
      "Gen Loss: 0.9243324 Disc Loss: 1.043944\n",
      "Gen Loss: 1.0811148 Disc Loss: 0.8556576\n",
      "Gen Loss: 1.3367484 Disc Loss: 0.71750236\n",
      "Gen Loss: 0.88825977 Disc Loss: 1.1174884\n",
      "Gen Loss: 1.0195497 Disc Loss: 0.93425333\n",
      "Gen Loss: 1.1794401 Disc Loss: 0.88808537\n",
      "Gen Loss: 0.7008979 Disc Loss: 1.8767948\n",
      "Gen Loss: 0.16815889 Disc Loss: 0.978528\n",
      "Gen Loss: 0.9425293 Disc Loss: 1.2471285\n",
      "Gen Loss: 1.0204117 Disc Loss: 1.0318127\n",
      "Gen Loss: 1.1102904 Disc Loss: 0.96808124\n",
      "Gen Loss: 1.2079538 Disc Loss: 0.7946365\n",
      "Gen Loss: 0.82612634 Disc Loss: 1.656563\n",
      "Gen Loss: 0.8775062 Disc Loss: 1.2262976\n",
      "Gen Loss: 0.92561376 Disc Loss: 1.2822652\n",
      "Gen Loss: 0.75785875 Disc Loss: 1.5601523\n",
      "Gen Loss: 0.9549346 Disc Loss: 0.9969171\n",
      "Gen Loss: 0.8773729 Disc Loss: 1.241716\n",
      "Gen Loss: 1.0762568 Disc Loss: 0.9843254\n",
      "Gen Loss: 1.2634087 Disc Loss: 0.78033257\n",
      "Gen Loss: 1.285163 Disc Loss: 0.71392894\n",
      "Gen Loss: 1.0428172 Disc Loss: 0.91550195\n",
      "Gen Loss: 0.97417164 Disc Loss: 0.97738755\n",
      "Gen Loss: 0.7937665 Disc Loss: 1.4021394\n",
      "Gen Loss: 0.6783048 Disc Loss: 1.5202401\n",
      "Gen Loss: 0.749122 Disc Loss: 1.3881218\n",
      "Gen Loss: 0.40569663 Disc Loss: 1.9231887\n",
      "Gen Loss: 0.6274343 Disc Loss: 1.6481402\n",
      "Gen Loss: 1.0043359 Disc Loss: 1.130615\n",
      "Gen Loss: 0.93171173 Disc Loss: 1.1626221\n",
      "Gen Loss: 0.9641212 Disc Loss: 1.0657812\n",
      "Gen Loss: 1.4328904 Disc Loss: 0.6376624\n",
      "Gen Loss: 1.0386689 Disc Loss: 1.487309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.9074469 Disc Loss: 1.0609739\n",
      "Gen Loss: 0.9198387 Disc Loss: 1.2088312\n",
      "Gen Loss: 0.9991252 Disc Loss: 1.1643431\n",
      "Gen Loss: 0.7917694 Disc Loss: 1.3904569\n",
      "Gen Loss: 0.7035321 Disc Loss: 1.6409004\n",
      "Gen Loss: 0.83441895 Disc Loss: 1.3532962\n",
      "Saved Model\n",
      "Gen Loss: 1.1762083 Disc Loss: 1.2123605\n",
      "Gen Loss: 1.0088532 Disc Loss: 1.2469879\n",
      "Gen Loss: 1.2679718 Disc Loss: 0.80648154\n",
      "Gen Loss: 1.0970814 Disc Loss: 1.0589557\n",
      "Gen Loss: 0.95330006 Disc Loss: 1.2923294\n",
      "Gen Loss: 0.6305063 Disc Loss: 1.7873483\n",
      "Gen Loss: 0.82391006 Disc Loss: 1.4337025\n",
      "Gen Loss: 1.1518941 Disc Loss: 0.8241776\n",
      "Gen Loss: 1.1374667 Disc Loss: 0.9001373\n",
      "Gen Loss: 0.8281247 Disc Loss: 1.0329378\n",
      "Gen Loss: 1.0976422 Disc Loss: 1.1588738\n",
      "Gen Loss: 1.1155415 Disc Loss: 0.9585097\n",
      "Gen Loss: 1.250243 Disc Loss: 0.7286284\n",
      "Gen Loss: 0.84312236 Disc Loss: 1.3911941\n",
      "Gen Loss: 0.93357825 Disc Loss: 1.2472101\n",
      "Gen Loss: 0.55481946 Disc Loss: 2.0557804\n",
      "Gen Loss: 0.7176522 Disc Loss: 1.5777302\n",
      "Gen Loss: 1.0714185 Disc Loss: 1.0415041\n",
      "Gen Loss: 1.1427168 Disc Loss: 0.91297424\n",
      "Gen Loss: 0.71481586 Disc Loss: 1.5394022\n",
      "Gen Loss: 0.8674865 Disc Loss: 1.3465114\n",
      "Gen Loss: 1.1966231 Disc Loss: 0.8536107\n",
      "Gen Loss: 0.8902128 Disc Loss: 1.1947892\n",
      "Gen Loss: 0.91261375 Disc Loss: 1.1631671\n",
      "Gen Loss: 0.69078684 Disc Loss: 1.6166419\n",
      "Gen Loss: 1.0386703 Disc Loss: 0.99336004\n",
      "Gen Loss: 1.1052581 Disc Loss: 0.8973544\n",
      "Gen Loss: 0.91216147 Disc Loss: 1.1348479\n",
      "Gen Loss: 1.1131401 Disc Loss: 0.90629613\n",
      "Gen Loss: 0.9110608 Disc Loss: 1.2169335\n",
      "Gen Loss: 0.54757977 Disc Loss: 1.9442087\n",
      "Gen Loss: 0.6034134 Disc Loss: 1.7125793\n",
      "Gen Loss: 0.84001136 Disc Loss: 1.239183\n",
      "Gen Loss: 0.97006905 Disc Loss: 0.86099327\n",
      "Gen Loss: 1.0308822 Disc Loss: 1.0227977\n",
      "Gen Loss: 0.94792056 Disc Loss: 1.2169276\n",
      "Gen Loss: 0.90208066 Disc Loss: 0.99542904\n",
      "Gen Loss: 0.7621796 Disc Loss: 1.2599092\n",
      "Gen Loss: 0.533468 Disc Loss: 1.2697649\n",
      "Gen Loss: 0.6439159 Disc Loss: 1.5005084\n",
      "Gen Loss: 0.8321024 Disc Loss: 1.3571839\n",
      "Gen Loss: 1.0067497 Disc Loss: 1.1361797\n",
      "Gen Loss: 0.77801263 Disc Loss: 1.5094397\n",
      "Gen Loss: 0.9970091 Disc Loss: 1.1810589\n",
      "Gen Loss: 1.3650303 Disc Loss: 0.78883284\n",
      "Gen Loss: 1.4496447 Disc Loss: 0.6030047\n",
      "Gen Loss: 0.8499847 Disc Loss: 1.4322715\n",
      "Gen Loss: 0.87006545 Disc Loss: 1.2958779\n",
      "Gen Loss: 1.1172814 Disc Loss: 1.009079\n",
      "Gen Loss: 0.88443345 Disc Loss: 1.2580454\n",
      "Gen Loss: 1.1935794 Disc Loss: 0.8522577\n",
      "Gen Loss: 0.85855204 Disc Loss: 1.1711527\n",
      "Gen Loss: 0.6367953 Disc Loss: 1.8050728\n",
      "Gen Loss: 0.6794488 Disc Loss: 1.7684562\n",
      "Gen Loss: 0.6699898 Disc Loss: 1.9446189\n",
      "Gen Loss: 1.057998 Disc Loss: 0.9491877\n",
      "Gen Loss: 1.1842537 Disc Loss: 0.81498265\n",
      "Gen Loss: 0.86103195 Disc Loss: 1.3218762\n",
      "Gen Loss: 1.1321809 Disc Loss: 0.88874435\n",
      "Gen Loss: 0.81819564 Disc Loss: 1.6101639\n",
      "Gen Loss: 0.6511358 Disc Loss: 1.590826\n",
      "Gen Loss: 0.75729114 Disc Loss: 1.4034188\n",
      "Gen Loss: 0.697653 Disc Loss: 1.5878551\n",
      "Gen Loss: 0.86160994 Disc Loss: 1.2141296\n",
      "Gen Loss: 0.98753965 Disc Loss: 1.0217085\n",
      "Gen Loss: 0.9070984 Disc Loss: 1.1371212\n",
      "Gen Loss: 1.2758703 Disc Loss: 0.8423767\n",
      "Gen Loss: 1.3278264 Disc Loss: 0.79437417\n",
      "Gen Loss: 1.1631491 Disc Loss: 1.1156654\n",
      "Gen Loss: 1.1914941 Disc Loss: 1.2954377\n",
      "Gen Loss: 1.3996285 Disc Loss: 0.8375864\n",
      "Gen Loss: 1.4930372 Disc Loss: 0.64233905\n",
      "Gen Loss: 1.2195123 Disc Loss: 0.9839576\n",
      "Gen Loss: 0.82900035 Disc Loss: 1.5278081\n",
      "Gen Loss: 0.8917322 Disc Loss: 1.4837322\n",
      "Gen Loss: 1.1369607 Disc Loss: 1.0657909\n",
      "Gen Loss: 0.7506484 Disc Loss: 1.132693\n",
      "Gen Loss: 0.62597436 Disc Loss: 1.8931296\n",
      "Gen Loss: 0.99866796 Disc Loss: 0.95610493\n",
      "Gen Loss: 0.6694146 Disc Loss: 1.6382747\n",
      "Gen Loss: 1.0766114 Disc Loss: 0.98544693\n",
      "Gen Loss: 1.4176189 Disc Loss: 0.73467326\n",
      "Gen Loss: 1.5121384 Disc Loss: 0.63512254\n",
      "Gen Loss: 1.4349191 Disc Loss: 1.1297129\n",
      "Gen Loss: 1.4660506 Disc Loss: 0.784572\n",
      "Gen Loss: 0.7775652 Disc Loss: 1.7360024\n",
      "Gen Loss: 0.74030066 Disc Loss: 1.8125397\n",
      "Gen Loss: 1.0258789 Disc Loss: 1.0727892\n",
      "Gen Loss: 0.819363 Disc Loss: 1.2867635\n",
      "Gen Loss: 0.8192701 Disc Loss: 1.2570682\n",
      "Gen Loss: 0.54888654 Disc Loss: 1.78788\n",
      "Gen Loss: 0.60541046 Disc Loss: 1.99904\n",
      "Gen Loss: 0.9371803 Disc Loss: 1.1211976\n",
      "Gen Loss: 1.1646754 Disc Loss: 0.96648127\n",
      "Gen Loss: 0.77732074 Disc Loss: 1.2795801\n",
      "Gen Loss: 0.8840027 Disc Loss: 1.152404\n",
      "Gen Loss: 0.7491728 Disc Loss: 1.4850838\n",
      "Gen Loss: 0.7780839 Disc Loss: 1.4443357\n",
      "Gen Loss: 0.72813046 Disc Loss: 1.4752784\n",
      "Gen Loss: 0.67578924 Disc Loss: 1.771732\n",
      "Saved Model\n",
      "Gen Loss: 0.6292859 Disc Loss: 1.6173248\n",
      "Gen Loss: 0.61149585 Disc Loss: 1.6542265\n",
      "Gen Loss: 0.71060205 Disc Loss: 1.5299867\n",
      "Gen Loss: 0.6638144 Disc Loss: 1.4794188\n",
      "Gen Loss: 0.7979977 Disc Loss: 1.4491334\n",
      "Gen Loss: 0.7077198 Disc Loss: 1.3597815\n",
      "Gen Loss: 0.99068 Disc Loss: 1.0078616\n",
      "Gen Loss: 0.76223546 Disc Loss: 1.4215713\n",
      "Gen Loss: 0.5166743 Disc Loss: 1.8716109\n",
      "Gen Loss: 0.5644276 Disc Loss: 1.8210087\n",
      "Gen Loss: 0.6546408 Disc Loss: 1.6965433\n",
      "Gen Loss: 0.73763865 Disc Loss: 1.4908082\n",
      "Gen Loss: 0.8012863 Disc Loss: 1.3498495\n",
      "Gen Loss: 0.67406726 Disc Loss: 1.6085836\n",
      "Gen Loss: 1.0706398 Disc Loss: 1.1071686\n",
      "Gen Loss: 0.9387695 Disc Loss: 1.0867627\n",
      "Gen Loss: 0.6816627 Disc Loss: 1.5028591\n",
      "Gen Loss: 0.6342083 Disc Loss: 1.5213987\n",
      "Gen Loss: 0.91342175 Disc Loss: 1.190104\n",
      "Gen Loss: 1.0842447 Disc Loss: 1.0362545\n",
      "Gen Loss: 0.9002454 Disc Loss: 1.2438388\n",
      "Gen Loss: 0.8914548 Disc Loss: 1.1757529\n",
      "Gen Loss: 0.9152268 Disc Loss: 1.1255997\n",
      "Gen Loss: 0.8446453 Disc Loss: 1.2338047\n",
      "Gen Loss: 0.5778944 Disc Loss: 1.799942\n",
      "Gen Loss: 0.7583174 Disc Loss: 1.4237823\n",
      "Gen Loss: 0.6946982 Disc Loss: 1.584892\n",
      "Gen Loss: 0.8311826 Disc Loss: 1.3051348\n",
      "Gen Loss: 0.62494236 Disc Loss: 1.660499\n",
      "Gen Loss: 0.5541081 Disc Loss: 1.8827016\n",
      "Gen Loss: 1.0107493 Disc Loss: 1.0837841\n",
      "Gen Loss: 1.2004141 Disc Loss: 0.9180385\n",
      "Gen Loss: 0.97844815 Disc Loss: 1.2165143\n",
      "Gen Loss: 1.4788648 Disc Loss: 0.86885226\n",
      "Gen Loss: 1.1606221 Disc Loss: 0.95982075\n",
      "Gen Loss: 0.65598905 Disc Loss: 1.6819365\n",
      "Gen Loss: 0.5793656 Disc Loss: 1.7501522\n",
      "Gen Loss: 0.54426044 Disc Loss: 1.8961613\n",
      "Gen Loss: 0.6960894 Disc Loss: 1.6210234\n",
      "Gen Loss: 0.6631454 Disc Loss: 1.5543966\n",
      "Gen Loss: 0.54613155 Disc Loss: 1.8449148\n",
      "Gen Loss: 0.5562998 Disc Loss: 1.8549478\n",
      "Gen Loss: 0.74398506 Disc Loss: 1.3943365\n",
      "Gen Loss: 0.8952008 Disc Loss: 1.1757709\n",
      "Gen Loss: 0.9456705 Disc Loss: 1.1185664\n",
      "Gen Loss: 1.1363974 Disc Loss: 0.9851539\n",
      "Gen Loss: 1.2331312 Disc Loss: 0.7164242\n",
      "Gen Loss: 0.8315759 Disc Loss: 1.3401154\n",
      "Gen Loss: 0.85117066 Disc Loss: 1.2733383\n",
      "Gen Loss: 0.8170686 Disc Loss: 1.2733644\n",
      "Gen Loss: 0.84636796 Disc Loss: 1.2185221\n",
      "Gen Loss: 0.81783813 Disc Loss: 1.330086\n",
      "Gen Loss: 0.71250165 Disc Loss: 1.3767298\n",
      "Gen Loss: 0.6998099 Disc Loss: 1.4773637\n",
      "Gen Loss: 0.7717923 Disc Loss: 1.3992257\n",
      "Gen Loss: 0.9335076 Disc Loss: 1.1448686\n",
      "Gen Loss: 0.8158244 Disc Loss: 1.4304348\n",
      "Gen Loss: 0.76631176 Disc Loss: 1.399949\n",
      "Gen Loss: 0.67200375 Disc Loss: 1.5500305\n",
      "Gen Loss: 0.74504864 Disc Loss: 1.3938072\n",
      "Gen Loss: 0.8400018 Disc Loss: 1.2663405\n",
      "Gen Loss: 1.043814 Disc Loss: 1.1028104\n",
      "Gen Loss: 0.6829942 Disc Loss: 1.3449736\n",
      "Gen Loss: 1.217871 Disc Loss: 0.8904013\n",
      "Gen Loss: 1.1041776 Disc Loss: 0.82407284\n",
      "Gen Loss: 0.9564125 Disc Loss: 1.2135873\n",
      "Gen Loss: 0.7522044 Disc Loss: 1.4597127\n",
      "Gen Loss: 0.7191627 Disc Loss: 1.5680505\n",
      "Gen Loss: 0.727358 Disc Loss: 1.443816\n",
      "Gen Loss: 0.8435642 Disc Loss: 1.3196706\n",
      "Gen Loss: 0.7253015 Disc Loss: 1.4684747\n",
      "Gen Loss: 0.9607083 Disc Loss: 1.1722124\n",
      "Gen Loss: 0.9529237 Disc Loss: 1.1195843\n",
      "Gen Loss: 0.73801446 Disc Loss: 1.4642168\n",
      "Gen Loss: 0.7824049 Disc Loss: 1.3699241\n",
      "Gen Loss: 0.87430143 Disc Loss: 1.1619394\n",
      "Gen Loss: 0.99374545 Disc Loss: 0.90949297\n",
      "Gen Loss: 0.8376012 Disc Loss: 1.276197\n",
      "Gen Loss: 0.9715116 Disc Loss: 1.1597015\n",
      "Gen Loss: 0.6208442 Disc Loss: 1.7873069\n",
      "Gen Loss: 0.7663431 Disc Loss: 1.3763974\n",
      "Gen Loss: 0.76392573 Disc Loss: 1.2807555\n",
      "Gen Loss: 0.78030646 Disc Loss: 1.5549433\n",
      "Gen Loss: 0.7537911 Disc Loss: 1.3638377\n",
      "Gen Loss: 1.0836291 Disc Loss: 0.96835136\n",
      "Gen Loss: 1.1632514 Disc Loss: 0.81222093\n",
      "Gen Loss: 1.2091243 Disc Loss: 0.9633364\n",
      "Gen Loss: 1.3419096 Disc Loss: 0.7331926\n",
      "Gen Loss: 1.4793186 Disc Loss: 0.6143246\n",
      "Gen Loss: 1.0780795 Disc Loss: 1.0506908\n",
      "Gen Loss: 0.65104294 Disc Loss: 1.6366892\n",
      "Gen Loss: 0.82101446 Disc Loss: 1.1540056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.70124817 Disc Loss: 1.5869023\n",
      "Gen Loss: 0.7187092 Disc Loss: 1.498436\n",
      "Gen Loss: 0.50055003 Disc Loss: 2.0917425\n",
      "Gen Loss: 0.7585918 Disc Loss: 1.461059\n",
      "Gen Loss: 1.0272427 Disc Loss: 0.9765278\n",
      "Gen Loss: 0.96405274 Disc Loss: 1.1410348\n",
      "Gen Loss: 0.8665427 Disc Loss: 1.0024085\n",
      "Gen Loss: 1.1451402 Disc Loss: 0.8668878\n",
      "Saved Model\n",
      "Gen Loss: 1.1754856 Disc Loss: 0.7536688\n",
      "Gen Loss: 0.73589176 Disc Loss: 1.4869044\n",
      "Gen Loss: 1.0241697 Disc Loss: 1.0836906\n",
      "Gen Loss: 1.0579455 Disc Loss: 1.0581551\n",
      "Gen Loss: 0.9458731 Disc Loss: 1.3616663\n",
      "Gen Loss: 1.0999631 Disc Loss: 1.4033644\n",
      "Gen Loss: 1.1532909 Disc Loss: 1.1907496\n",
      "Gen Loss: 0.24719647 Disc Loss: 1.9147636\n",
      "Gen Loss: 4.49517 Disc Loss: 0.03540604\n",
      "Gen Loss: 3.0417604 Disc Loss: 0.7151067\n",
      "Gen Loss: 1.6318784 Disc Loss: 0.55797136\n",
      "Gen Loss: 1.0934161 Disc Loss: 0.895334\n",
      "Gen Loss: 0.592941 Disc Loss: 1.5213017\n",
      "Gen Loss: 0.8975787 Disc Loss: 1.4212105\n",
      "Gen Loss: 1.0911677 Disc Loss: 0.96309507\n",
      "Gen Loss: 1.1764859 Disc Loss: 0.87823653\n",
      "Gen Loss: 0.80075115 Disc Loss: 1.5444117\n",
      "Gen Loss: 0.6681882 Disc Loss: 1.8617704\n",
      "Gen Loss: 1.1008093 Disc Loss: 1.19396\n",
      "Gen Loss: 0.7223078 Disc Loss: 1.3635101\n",
      "Gen Loss: 0.8759771 Disc Loss: 1.0370927\n",
      "Gen Loss: 0.81144565 Disc Loss: 1.1777167\n",
      "Gen Loss: 0.50298405 Disc Loss: 1.8350561\n",
      "Gen Loss: 0.8048082 Disc Loss: 1.436564\n",
      "Gen Loss: 0.75965416 Disc Loss: 1.3836832\n",
      "Gen Loss: 0.98992276 Disc Loss: 1.0814809\n",
      "Gen Loss: 1.0647783 Disc Loss: 0.98588383\n",
      "Gen Loss: 1.1649995 Disc Loss: 0.86445665\n",
      "Gen Loss: 0.804359 Disc Loss: 1.5319188\n",
      "Gen Loss: 0.8852667 Disc Loss: 1.4651129\n",
      "Gen Loss: 0.82012117 Disc Loss: 1.2300448\n",
      "Gen Loss: 1.0552962 Disc Loss: 1.0847899\n",
      "Gen Loss: 1.2566631 Disc Loss: 0.7719573\n",
      "Gen Loss: 0.8655326 Disc Loss: 1.2602572\n",
      "Gen Loss: 0.8881974 Disc Loss: 1.218574\n",
      "Gen Loss: 0.59262526 Disc Loss: 1.8762159\n",
      "Gen Loss: 0.92097366 Disc Loss: 1.1555637\n",
      "Gen Loss: 1.0633163 Disc Loss: 0.9721869\n",
      "Gen Loss: 0.75681555 Disc Loss: 1.4340603\n",
      "Gen Loss: 0.8973559 Disc Loss: 1.514797\n",
      "Gen Loss: 0.72292054 Disc Loss: 1.4082273\n",
      "Gen Loss: 0.93179893 Disc Loss: 1.0937426\n",
      "Gen Loss: 0.8099857 Disc Loss: 1.4657187\n",
      "Gen Loss: 1.1581424 Disc Loss: 0.8611703\n",
      "Gen Loss: 1.2056742 Disc Loss: 0.7718252\n",
      "Gen Loss: 1.4514523 Disc Loss: 0.5758132\n",
      "Gen Loss: 1.4133089 Disc Loss: 0.67772543\n",
      "Gen Loss: 1.5038514 Disc Loss: 0.5363642\n",
      "Gen Loss: 0.7844997 Disc Loss: 1.2688812\n",
      "Gen Loss: 0.7979561 Disc Loss: 1.5033741\n",
      "Gen Loss: 1.1031033 Disc Loss: 0.90113765\n",
      "Gen Loss: 0.7624138 Disc Loss: 1.0787456\n",
      "Gen Loss: 0.9876087 Disc Loss: 1.2449492\n",
      "Gen Loss: 0.59745264 Disc Loss: 1.7965218\n",
      "Gen Loss: 0.8992895 Disc Loss: 1.1393526\n",
      "Gen Loss: 1.1299975 Disc Loss: 0.9183409\n",
      "Gen Loss: 1.0540481 Disc Loss: 1.0343978\n",
      "Gen Loss: 0.8924105 Disc Loss: 1.001495\n",
      "Gen Loss: 0.8785101 Disc Loss: 1.2930175\n",
      "Gen Loss: 1.2785265 Disc Loss: 0.71700037\n",
      "Gen Loss: 1.1471695 Disc Loss: 0.91439867\n",
      "Gen Loss: 1.2328012 Disc Loss: 1.192285\n",
      "Gen Loss: 0.8920841 Disc Loss: 1.4161464\n",
      "Gen Loss: 1.3629415 Disc Loss: 0.69013345\n",
      "Gen Loss: 1.3936286 Disc Loss: 0.67617685\n",
      "Gen Loss: 1.5256984 Disc Loss: 0.6030543\n",
      "Gen Loss: 1.516819 Disc Loss: 0.62204427\n",
      "Gen Loss: 1.0162787 Disc Loss: 1.3371143\n",
      "Gen Loss: 1.317733 Disc Loss: 0.89526606\n",
      "Gen Loss: 1.0040966 Disc Loss: 0.91139686\n",
      "Gen Loss: 0.9872689 Disc Loss: 0.956685\n",
      "Gen Loss: 0.85551965 Disc Loss: 1.2133358\n",
      "Gen Loss: 1.0023184 Disc Loss: 1.0285153\n",
      "Gen Loss: 1.1310922 Disc Loss: 0.89230645\n",
      "Gen Loss: 1.1566995 Disc Loss: 1.143292\n",
      "Gen Loss: 0.74431616 Disc Loss: 1.7155418\n",
      "Gen Loss: 0.8059521 Disc Loss: 1.3899076\n",
      "Gen Loss: 0.7852469 Disc Loss: 1.503473\n",
      "Gen Loss: 0.9566926 Disc Loss: 1.0451496\n",
      "Gen Loss: 0.9539107 Disc Loss: 1.2077065\n",
      "Gen Loss: 0.99740565 Disc Loss: 1.0440075\n",
      "Gen Loss: 0.8599464 Disc Loss: 1.3097641\n",
      "Gen Loss: 0.946222 Disc Loss: 1.0476246\n",
      "Gen Loss: 1.0286031 Disc Loss: 0.9917525\n",
      "Gen Loss: 0.9110366 Disc Loss: 1.4031582\n",
      "Gen Loss: 1.0470073 Disc Loss: 0.9699999\n",
      "Gen Loss: 0.8434149 Disc Loss: 1.2691605\n",
      "Gen Loss: 1.1996402 Disc Loss: 0.9339628\n",
      "Gen Loss: 0.5515237 Disc Loss: 2.8117418\n",
      "Gen Loss: 1.0980775 Disc Loss: 1.086997\n",
      "Gen Loss: 1.12451 Disc Loss: 1.0448819\n",
      "Gen Loss: 1.6416135 Disc Loss: 0.52206683\n",
      "Gen Loss: 2.039789 Disc Loss: 0.3355614\n",
      "Gen Loss: 2.0704052 Disc Loss: 0.29607713\n",
      "Gen Loss: 2.4430566 Disc Loss: 0.22996557\n",
      "Gen Loss: 2.8027182 Disc Loss: 0.14765736\n",
      "Gen Loss: 2.7997894 Disc Loss: 0.14443612\n",
      "Gen Loss: 3.071963 Disc Loss: 0.11908134\n",
      "Gen Loss: 3.193565 Disc Loss: 0.100608625\n",
      "Gen Loss: 3.3437548 Disc Loss: 0.079042755\n",
      "Saved Model\n",
      "Gen Loss: 3.387414 Disc Loss: 0.09770812\n",
      "Gen Loss: 3.4793735 Disc Loss: 0.069838546\n",
      "Gen Loss: 1.9744933 Disc Loss: 0.53205454\n",
      "Gen Loss: 0.75948817 Disc Loss: 1.7705659\n",
      "Gen Loss: 0.66890043 Disc Loss: 2.0351515\n",
      "Gen Loss: 0.758757 Disc Loss: 1.587467\n",
      "Gen Loss: 1.083497 Disc Loss: 0.97128665\n",
      "Gen Loss: 1.1109762 Disc Loss: 0.8973484\n",
      "Gen Loss: 1.2267889 Disc Loss: 1.0580671\n",
      "Gen Loss: 0.65785396 Disc Loss: 2.0498385\n",
      "Gen Loss: 0.70182794 Disc Loss: 1.4709433\n",
      "Gen Loss: 0.74740285 Disc Loss: 1.3792434\n",
      "Gen Loss: 1.040168 Disc Loss: 0.9825604\n",
      "Gen Loss: 0.8575989 Disc Loss: 1.1810707\n",
      "Gen Loss: 1.0807414 Disc Loss: 0.9371966\n",
      "Gen Loss: 1.3349403 Disc Loss: 0.6827476\n",
      "Gen Loss: 1.3447285 Disc Loss: 0.6326571\n",
      "Gen Loss: 1.1693509 Disc Loss: 0.8297043\n",
      "Gen Loss: 1.003295 Disc Loss: 1.3763555\n",
      "Gen Loss: 0.7098932 Disc Loss: 1.4981033\n",
      "Gen Loss: 0.7078185 Disc Loss: 1.5078979\n",
      "Gen Loss: 0.48488477 Disc Loss: 2.2172399\n",
      "Gen Loss: 0.93128324 Disc Loss: 1.1296158\n",
      "Gen Loss: 0.9669871 Disc Loss: 1.3405781\n",
      "Gen Loss: 1.2523124 Disc Loss: 1.0732701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-b80f3e253fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator, twice for good measure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gen Loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgLoss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Disc Loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 500000 #Total number of iterations to use.\n",
    "sample_directory = './CDCGANfigs' #Directory to save sample images from generator in.\n",
    "model_directory = './CDCGANmodels' #Directory to save trained model to.\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs, c_in:xs}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs, c_in:xs}) #Update the generator, twice for good measure.\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs, c_in:xs})\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        if i % 10 == 0:\n",
    "            print(\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2, c_in:xs}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "batch_size_sample = 36\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print 'Loading Model...'\n",
    "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    zs = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32) #Generate a random z batch\n",
    "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
